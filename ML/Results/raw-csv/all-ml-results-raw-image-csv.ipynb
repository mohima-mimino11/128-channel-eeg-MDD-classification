{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-03T09:42:43.573120Z",
     "iopub.status.busy": "2025-12-03T09:42:43.572786Z",
     "iopub.status.idle": "2025-12-03T09:42:43.689690Z",
     "shell.execute_reply": "2025-12-03T09:42:43.688633Z",
     "shell.execute_reply.started": "2025-12-03T09:42:43.573090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010030rest 20160324 1054..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020025rest 20150713 1519..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010013rest 20150703 1333..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020016rest 20150701 1040..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020015_rest 20150630 1527.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010022restnew 20150724 14.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020027rest 20150713 1049..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010008_rest 20150619 1653.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010011rest 20150625 1516..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020029rest 20150715 1316..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020023restnew 20150709 10.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010012rest 20150626 1026..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030020_rest 20151230 1416.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010018rest 20150716 1237..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010002rest 20150416 1017..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020010rest 20150625 1224..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010016rest 20150710 1220..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020008rest 20150624 1711..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010024rest 20150814 1504..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030004_rest 20151026 1930.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020020rest 20150703 1754..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020018rest 20150702 1651..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010010rest 20150624 1447..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030002rest_new 20151022 1.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010033rest 20160331 1239..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020019rest 20150703 1036..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010005rest 20150507 0907..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030007_rest 20151103 2032.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030017_rest 20151208 1329.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030018_rest 20151208 1443.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020013rest 20150629 1607..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010026rest 20160311 1421..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030006_rest 20151103 1725.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010023rest 20150729 1929..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020021rest 20150707 1720..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020026_rest 20150714 1413.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030019_rest 20151230 1314.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030005rest 20151026 2103..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010015rest 20150709 1456..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030009_rest 20151105 1113.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010006rest 20150528 0928..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030003_rest 20151022 1155.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010004rest 20150427 1335..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020022rest 20150707 1452..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030021rest 20160105 1141..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010036_rest 20160408 1418.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010034rest 20160407 0938..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010028rest 20160317 1538..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030014rest 20151117 1441..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010019rest 20150716 1440..csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020014_rest 20150630 1023.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030017erp 20151208 1351-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010002erp 20150416 1131-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010011erp 20150625 1545-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010024erp 20150814 1523-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010018erp 20150716 1310-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010010erp 20150624 1508-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020025_erp 20150713 1541-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010030erp 20160324 0915-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030021erp 20160105 1204-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020020_erp 20150703 1810-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020027_erp 20150713 1116-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030020_erp 20151230 1443-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010015erp 20150709 1534-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010013erp 20150703 1353-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010023erp 20150729 1955-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030009erp 20151105 1207-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010025erp 20160311 1225-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010021erp 20150805 1818-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020016_erp 20150701 1054-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010034erp 20160407 0959-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020019_erp 20150703 1052-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030005erp 20151026 2116-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020013_erp 20150629 1622-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020029_erp 20150715 1401-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010019erp 20150716 1544-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020026_erp 20150714 1428-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010033erp 20160331 1307-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010005erp 20150507 0938-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010006erp 20150528 1007-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030019_erp 20151230 1331-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030006 20151103 1801-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010004erp 20141219 1602-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020010_erp 20150625 1244-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030007 20151103 2129-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030014erp 20151117 1419-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030002erp_new 20151022 14-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020015_erp 20150630 1547-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020021_erp 20150707 1751-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010012erp 20150626 1059-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010028erp 20160317 1554-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020022_erp 20150707 1515-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020018_erp 20150702 1721-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020023_erpnew 20150709 110-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010026erp 20160311 1441-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010016erp 20150710 1329-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010008_erp(n) 20150619 1709-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020008_erp 20150624 1740-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010036erp 20160408 1452-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010022erp 20150724 1457-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020014_erp 20150630 1052-preprocessed.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/18.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/20.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/07.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/24.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/11.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/17.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/16.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/19.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/26.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/28.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/13.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/23.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/14.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/22.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/06.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/05.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/12.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/09.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/04.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/01.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/03.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/15.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/all_audio_features.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/10.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/08.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/25.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/29.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/21.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/27.csv\n",
      "/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/02.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAW CSV USING leave-one-out cross-validation with paramter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T08:53:01.885136Z",
     "iopub.status.busy": "2025-08-27T08:53:01.884670Z",
     "iopub.status.idle": "2025-08-27T09:21:02.326776Z",
     "shell.execute_reply": "2025-08-27T09:21:02.324597Z",
     "shell.execute_reply.started": "2025-08-27T08:53:01.885112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, sampling, and labeling data...\n",
      "Loaded 2434650 rows of data from 3 unique subjects.\n",
      "\n",
      "--- Verifying the Combined and Labeled DataFrame ---\n",
      "First 5 rows of the dataset:\n",
      "         E1        E2        E3        E4            E5        E6  \\\n",
      "0  0.000004 -0.000004 -0.000010 -0.000004  4.862534e-06 -0.000009   \n",
      "1  0.000009 -0.000008 -0.000006 -0.000002 -9.369657e-07  0.000002   \n",
      "2  0.000003 -0.000004 -0.000009 -0.000002  9.702576e-08  0.000003   \n",
      "3 -0.000008 -0.000002 -0.000007 -0.000001 -3.968677e-05  0.000007   \n",
      "4 -0.000016 -0.000021 -0.000003 -0.000002  4.623818e-06 -0.000003   \n",
      "\n",
      "             E7        E8        E9       E10  ...      E122      E123  \\\n",
      "0  1.255296e-07 -0.000012 -0.000010 -0.000009  ...  0.000010 -0.000011   \n",
      "1  2.801928e-06 -0.000012 -0.000008 -0.000006  ... -0.000009 -0.000010   \n",
      "2  4.612941e-06 -0.000014 -0.000010 -0.000011  ... -0.000004  0.000002   \n",
      "3  1.789049e-06 -0.000012  0.000003  0.000003  ... -0.000010 -0.000004   \n",
      "4  2.116167e-06 -0.000019 -0.000007 -0.000023  ...  0.000010 -0.000007   \n",
      "\n",
      "           E124      E125      E126          E127      E128     time  label  \\\n",
      "0 -1.448931e-06  0.000002  0.000006  3.506232e-07  0.000002  528.816      0   \n",
      "1 -1.230388e-06 -0.000004  0.000003 -5.146654e-06 -0.000003  787.804      0   \n",
      "2 -4.229560e-07  0.000002 -0.000004 -2.822239e-06 -0.000001  592.900      0   \n",
      "3  9.433372e-06 -0.000006  0.000004  3.604502e-06  0.000005  700.396      0   \n",
      "4 -9.814793e-06 -0.000012 -0.000003  1.916255e-06  0.000003  537.196      0   \n",
      "\n",
      "   subject  \n",
      "0     0203  \n",
      "1     0203  \n",
      "2     0203  \n",
      "3     0203  \n",
      "4     0203  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "\n",
      "Last 5 rows of the dataset:\n",
      "               E1        E2        E3        E4        E5        E6  \\\n",
      "2434645  0.000006 -0.000015 -0.000011 -0.000007 -0.000008 -0.000008   \n",
      "2434646 -0.000013  0.000024  0.000026  0.000025  0.000034  0.000038   \n",
      "2434647  0.000010 -0.000008 -0.000017 -0.000014 -0.000014 -0.000015   \n",
      "2434648  0.000003  0.000005  0.000003  0.000004  0.000005  0.000006   \n",
      "2434649 -0.000006  0.000001  0.000006  0.000007  0.000005  0.000002   \n",
      "\n",
      "                   E7        E8            E9       E10  ...      E122  \\\n",
      "2434645 -2.981412e-06 -0.000010 -1.655648e-05 -0.000011  ... -0.000006   \n",
      "2434646  7.787202e-06  0.000157  1.127090e-04  0.000063  ... -0.000016   \n",
      "2434647 -3.672151e-06 -0.000011 -9.753262e-06 -0.000019  ...  0.000003   \n",
      "2434648  4.208531e-06  0.000001  3.437439e-07  0.000009  ...  0.000004   \n",
      "2434649 -8.332136e-07 -0.000008  9.647875e-07  0.000004  ...  0.000003   \n",
      "\n",
      "                 E123          E124      E125      E126      E127      E128  \\\n",
      "2434645 -4.315159e-06 -3.733152e-06  0.000009  0.000014  0.000008  0.000014   \n",
      "2434646  3.107228e-06  9.763512e-06 -0.000027 -0.000047 -0.000034 -0.000009   \n",
      "2434647 -9.511130e-06 -1.045835e-05  0.000014  0.000021  0.000017  0.000008   \n",
      "2434648  6.088985e-07  1.881065e-07  0.000002  0.000003 -0.000002 -0.000003   \n",
      "2434649  3.487086e-06  3.848203e-06 -0.000004 -0.000006  0.000008  0.000007   \n",
      "\n",
      "            time  label  subject  \n",
      "2434645  159.948      1     0202  \n",
      "2434646  730.972      1     0202  \n",
      "2434647  582.900      1     0202  \n",
      "2434648  598.436      1     0202  \n",
      "2434649  384.072      1     0202  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "\n",
      "Distribution of Labels (0=Healthy, 1=Depressed):\n",
      "label\n",
      "1    1932275\n",
      "0     502375\n",
      "Name: count, dtype: int64\n",
      "--- Verification Complete ---\n",
      "\n",
      "Found 334 missing values. Imputing with column means.\n",
      "\n",
      "--- Starting Hyperparameter Tuning ---\n",
      "Tuning Random Forest...\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/22304812.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     random_search = RandomizedSearchCV(estimator=model_instance, param_distributions=param_dist[name], \n\u001b[1;32m    102\u001b[0m                                        n_iter=10, cv=2, scoring='accuracy', n_jobs=-1, random_state=42, verbose=1)\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best params for {name}: {best_params[name]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, LeaveOneGroupOut, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- 1. SETUP AND DATA LOADING ---\n",
    "directory_path = '/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1'\n",
    "output_dir = '/kaggle/working'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "subject_labels = {\n",
    "    # --- Depressed (MDD) Patients - Label 1 ---\n",
    "    '0201': 1, '0202': 1, # Add all MDD subject ID prefixes here\n",
    "    \n",
    "    # --- Healthy Controls (HC) - Label 0 ---\n",
    "    '0203': 0, '0204': 0, # Add all HC subject ID prefixes here\n",
    "}\n",
    "\n",
    "print(\"Loading, sampling, and labeling data...\")\n",
    "data_frames = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    subject_prefix = filename[:4]\n",
    "    if subject_prefix in subject_labels:\n",
    "        data = pd.read_csv(os.path.join(directory_path, filename))\n",
    "        data['label'] = subject_labels[subject_prefix]\n",
    "        data['subject'] = subject_prefix\n",
    "        sample = data.sample(frac=0.25, random_state=42)\n",
    "        data_frames.append(sample)\n",
    "\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "num_subjects = len(combined_data['subject'].unique())\n",
    "print(f\"Loaded {combined_data.shape[0]} rows of data from {num_subjects} unique subjects.\")\n",
    "\n",
    "# --- NEW CODE BLOCK: VERIFY THE LABELED DATA ---\n",
    "print(\"\\n--- Verifying the Combined and Labeled DataFrame ---\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(combined_data.head())\n",
    "print(\"\\nLast 5 rows of the dataset:\")\n",
    "print(combined_data.tail())\n",
    "print(\"\\nDistribution of Labels (0=Healthy, 1=Depressed):\")\n",
    "print(combined_data['label'].value_counts())\n",
    "print(\"--- Verification Complete ---\\n\")\n",
    "# --- END OF NEW CODE BLOCK ---\n",
    "\n",
    "if 'time' in combined_data.columns: combined_data.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# --- 2. FEATURE/TARGET DEFINITION AND PREPROCESSING ---\n",
    "y = combined_data['label']\n",
    "groups = combined_data['subject']\n",
    "X = combined_data.drop(columns=['label', 'subject'])\n",
    "\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"Found {X.isnull().sum().sum()} missing values. Imputing with column means.\")\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "# --- 3. HYPERPARAMETER TUNING ---\n",
    "print(\"\\n--- Starting Hyperparameter Tuning ---\")\n",
    "param_dist = {\n",
    "    'KNN': {'pca__n_components': randint(20, 50), 'knn__n_neighbors': randint(3, 20)},\n",
    "    'Random Forest': {'n_estimators': randint(100, 500), 'max_depth': [10, 20, 30, None]},\n",
    "    'XGBoost': {'n_estimators': randint(100, 500), 'learning_rate': uniform(0.01, 0.2), 'max_depth': randint(3, 10)},\n",
    "    'Linear SVM': {'base_estimator__C': uniform(0.1, 10)},\n",
    "    'Logistic Regression': {'C': uniform(0.1, 10)}\n",
    "}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "best_params = {}\n",
    "models_to_tune = [ 'Random Forest', 'XGBoost', 'Linear SVM', 'Logistic Regression']\n",
    "\n",
    "for name in models_to_tune:\n",
    "    print(f\"Tuning {name}...\")\n",
    "    if name == 'KNN':\n",
    "        model_instance = Pipeline([('pca', PCA()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n",
    "    else:\n",
    "        model_instance = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1),\n",
    "        'Linear SVM': CalibratedClassifierCV(LinearSVC(random_state=42, dual=False, max_iter=3000)),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "    }[name]\n",
    "    \n",
    "    X_train_tune, _, y_train_tune, _ = train_test_split(X_scaled, y, train_size=0.5, stratify=y, random_state=42)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(estimator=model_instance, param_distributions=param_dist[name], \n",
    "                                       n_iter=10, cv=2, scoring='accuracy', n_jobs=-1, random_state=42, verbose=1)\n",
    "    random_search.fit(X_train_tune, y_train_tune)\n",
    "    best_params[name] = random_search.best_params_\n",
    "    print(f\"Best params for {name}: {best_params[name]}\")\n",
    "\n",
    "# --- 4. DEFINE MODELS WITH TUNED PARAMETERS ---\n",
    "# tuned_knn_pipeline = Pipeline([\n",
    "#     ('pca', PCA(n_components=best_params.get('KNN', {}).get('pca__n_components', 30))),\n",
    "#     ('knn', KNeighborsClassifier(n_neighbors=best_params.get('KNN', {}).get('knn__n_neighbors', 5), n_jobs=-1))\n",
    "# ])\n",
    "\n",
    "tuned_models = {\n",
    "    'Tuned KNN (PCA)': tuned_knn_pipeline,\n",
    "    'Tuned Linear SVM': CalibratedClassifierCV(LinearSVC(C=best_params.get('Linear SVM', {}).get('base_estimator__C', 1.0), random_state=42, dual=False, max_iter=3000)),\n",
    "    'Tuned RF': RandomForestClassifier(random_state=42, n_jobs=-1, **best_params.get('Random Forest', {})),\n",
    "    'Tuned XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, **best_params.get('XGBoost', {})),\n",
    "    'Tuned LogReg': LogisticRegression(max_iter=1000, random_state=42, **best_params.get('Logistic Regression', {})),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'LDA': LinearDiscriminantAnalysis()\n",
    "}\n",
    "cmap = plt.colormaps.get('tab10')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(tuned_models))]\n",
    "\n",
    "# --- 5. GENERATE PLOTS FROM A SINGLE TRAIN-TEST SPLIT (for visualization) ---\n",
    "print(\"\\n--- Generating Reports and ROC Curves from a single split ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "fig_reports, axes_reports = plt.subplots(4, 2, figsize=(18, 24)); axes_reports = axes_reports.flatten()\n",
    "fig_reports.suptitle('Classification Reports for All Models (Raw EEG Data)', fontsize=24, y=1.0)\n",
    "if len(tuned_models) % 2 != 0: fig_reports.delaxes(axes_reports[-1])\n",
    "\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(12, 10))\n",
    "ax_roc.set_title('ROC Curves for Depression Classification (Raw EEG Data)', fontsize=16)\n",
    "\n",
    "for i, (model_name, model) in enumerate(tuned_models.items()):\n",
    "    print(f\"Fitting {model_name} for plotting...\")\n",
    "    pipeline_plot = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    pipeline_plot.fit(X_train, y_train)\n",
    "    y_pred = pipeline_plot.predict(X_test)\n",
    "    \n",
    "    report_text = classification_report(y_test, y_pred, target_names=['Healthy', 'Depressed'])\n",
    "    axes_reports[i].axis('off'); axes_reports[i].set_title(model_name, fontsize=16, pad=20)\n",
    "    axes_reports[i].text(0.01, 0.95, report_text, family='monospace', fontsize=14, va='top')\n",
    "    \n",
    "    if hasattr(pipeline_plot, \"predict_proba\"):\n",
    "        y_proba = pipeline_plot.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        ax_roc.plot(fpr, tpr, color=colors[i], lw=2.5, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "fig_reports.tight_layout(pad=3.0)\n",
    "fig_reports.savefig(os.path.join(output_dir, \"all_classification_reports_raw_csv.png\"))\n",
    "plt.close(fig_reports)\n",
    "print(f\"Saved consolidated classification reports to: {os.path.join(output_dir, 'all_classification_reports_raw_csv.png')}\")\n",
    "\n",
    "ax_roc.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Guess')\n",
    "ax_roc.legend(loc='lower right', fontsize=12)\n",
    "ax_roc.grid(alpha=0.5)\n",
    "fig_roc.savefig(os.path.join(output_dir, \"all_roc_curves_raw_csv.png\"))\n",
    "plt.close(fig_roc)\n",
    "print(f\"Saved consolidated ROC curves to: {os.path.join(output_dir, 'all_roc_curves_raw_csv.png')}\")\n",
    "\n",
    "# --- 6. LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION ---\n",
    "print(\"\\n--- Performing Leave-One-Subject-Out Cross-Validation ---\")\n",
    "logo = LeaveOneGroupOut()\n",
    "cv_scores = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    print(f\"Evaluating {model_name} with LOSO-CV...\")\n",
    "    pipeline_cv = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    scores = cross_val_score(pipeline_cv, X, y, cv=logo, groups=groups, scoring='accuracy', n_jobs=-1)\n",
    "    cv_scores[model_name] = scores\n",
    "    print(f\"{model_name} Mean Accuracy: {np.mean(scores):.3f} Â± {np.std(scores):.3f}\")\n",
    "\n",
    "# --- 7. VISUALIZE LOSO-CV RESULTS ---\n",
    "fig_cv, ax_cv = plt.subplots(figsize=(14, 9))\n",
    "model_names = list(cv_scores.keys())\n",
    "mean_accuracies = [np.mean(s) for s in cv_scores.values()]\n",
    "std_devs = [np.std(s) for s in cv_scores.values()]\n",
    "ax_cv.bar(model_names, mean_accuracies, yerr=std_devs, capsize=5, color=colors, alpha=0.8)\n",
    "ax_cv.set_title(f'Leave-One-Subject-Out Cross-Validation Accuracy (n={num_subjects} subjects)', fontsize=16)\n",
    "ax_cv.set_ylabel('Mean Accuracy', fontsize=14); ax_cv.set_ylim(0, 1.05)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "for i, acc in enumerate(mean_accuracies): ax_cv.text(i, acc + 0.05, f\"{acc:.3f}\", ha='center')\n",
    "fig_cv.tight_layout()\n",
    "fig_cv.savefig(os.path.join(output_dir, \"loso_cv_accuracy_barchart.png\"))\n",
    "plt.close(fig_cv)\n",
    "print(f\"Saved LOSO-CV results chart to: {os.path.join(output_dir, 'loso_cv_accuracy_barchart.png')}\")\n",
    "\n",
    "# --- 8. GENERATE LEARNING CURVES ---\n",
    "print(\"\\n--- Generating Learning Curves ---\")\n",
    "fig_lc, axes_lc = plt.subplots(4, 2, figsize=(18, 24), sharey=True)\n",
    "if len(tuned_models) % 2 != 0: fig_lc.delaxes(axes_lc.flatten()[-1])\n",
    "axes_lc = axes_lc.flatten()\n",
    "fig_lc.suptitle('Learning Curves for All Models', fontsize=24, y=1.0)\n",
    "train_sizes_abs = np.linspace(0.1, 1.0, 7)\n",
    "\n",
    "for i, (model_name, model) in enumerate(tuned_models.items()):\n",
    "    print(f\"Generating learning curve for {model_name}...\")\n",
    "    pipeline_lc = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        pipeline_lc, X, y, cv=5, n_jobs=-1, train_sizes=train_sizes_abs, scoring='accuracy')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    \n",
    "    ax = axes_lc[i]\n",
    "    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    ax.set_title(model_name, fontsize=16)\n",
    "    ax.set_xlabel(\"Training examples\"); ax.set_ylabel(\"Accuracy\")\n",
    "    ax.grid(); ax.legend(loc=\"best\")\n",
    "\n",
    "fig_lc.tight_layout(pad=3.0)\n",
    "fig_lc.savefig(os.path.join(output_dir, \"all_learning_curves_raw_csv.png\"))\n",
    "plt.close(fig_lc)\n",
    "print(f\"Saved learning curves to: {os.path.join(output_dir, 'all_learning_curves_raw_csv.png')}\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-27T11:36:08.744Z",
     "iopub.execute_input": "2025-08-27T09:23:03.656352Z",
     "iopub.status.busy": "2025-08-27T09:23:03.656078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, sampling, and labeling data...\n",
      "Loaded 973858 rows of data from 3 unique subjects.\n",
      "\n",
      "--- Verifying the Combined and Labeled DataFrame ---\n",
      "First 5 rows of the dataset:\n",
      "         E1        E2        E3        E4            E5        E6  \\\n",
      "0  0.000004 -0.000004 -0.000010 -0.000004  4.862534e-06 -0.000009   \n",
      "1  0.000009 -0.000008 -0.000006 -0.000002 -9.369657e-07  0.000002   \n",
      "2  0.000003 -0.000004 -0.000009 -0.000002  9.702576e-08  0.000003   \n",
      "3 -0.000008 -0.000002 -0.000007 -0.000001 -3.968677e-05  0.000007   \n",
      "4 -0.000016 -0.000021 -0.000003 -0.000002  4.623818e-06 -0.000003   \n",
      "\n",
      "             E7        E8        E9       E10  ...      E122      E123  \\\n",
      "0  1.255296e-07 -0.000012 -0.000010 -0.000009  ...  0.000010 -0.000011   \n",
      "1  2.801928e-06 -0.000012 -0.000008 -0.000006  ... -0.000009 -0.000010   \n",
      "2  4.612941e-06 -0.000014 -0.000010 -0.000011  ... -0.000004  0.000002   \n",
      "3  1.789049e-06 -0.000012  0.000003  0.000003  ... -0.000010 -0.000004   \n",
      "4  2.116167e-06 -0.000019 -0.000007 -0.000023  ...  0.000010 -0.000007   \n",
      "\n",
      "           E124      E125      E126          E127      E128     time  label  \\\n",
      "0 -1.448931e-06  0.000002  0.000006  3.506232e-07  0.000002  528.816      0   \n",
      "1 -1.230388e-06 -0.000004  0.000003 -5.146654e-06 -0.000003  787.804      0   \n",
      "2 -4.229560e-07  0.000002 -0.000004 -2.822239e-06 -0.000001  592.900      0   \n",
      "3  9.433372e-06 -0.000006  0.000004  3.604502e-06  0.000005  700.396      0   \n",
      "4 -9.814793e-06 -0.000012 -0.000003  1.916255e-06  0.000003  537.196      0   \n",
      "\n",
      "   subject  \n",
      "0     0203  \n",
      "1     0203  \n",
      "2     0203  \n",
      "3     0203  \n",
      "4     0203  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "\n",
      "Last 5 rows of the dataset:\n",
      "              E1            E2            E3            E4            E5  \\\n",
      "973853 -0.000003  4.979376e-06  7.114335e-07  5.315793e-07 -2.077677e-06   \n",
      "973854  0.000006  1.581409e-05  7.864800e-06  4.872604e-06  4.100140e-06   \n",
      "973855  0.000007 -6.264312e-07 -1.662009e-07  6.271635e-07 -4.571024e-07   \n",
      "973856  0.000006 -7.671812e-06 -1.250372e-05 -7.933358e-06 -3.067835e-06   \n",
      "973857  0.000028  3.715436e-05  2.986131e-05  2.431463e-05  1.500560e-05   \n",
      "\n",
      "                  E6            E7        E8            E9       E10  ...  \\\n",
      "973853 -5.631489e-06 -6.806103e-07 -0.000007 -6.096326e-06  0.000002  ...   \n",
      "973854  8.363340e-07 -3.500423e-06  0.000045  2.659136e-05  0.000014  ...   \n",
      "973855 -2.805155e-06  1.278018e-06  0.000005  2.462231e-08 -0.000002  ...   \n",
      "973856 -3.902949e-06 -9.218238e-07  0.000004  2.211063e-06 -0.000006  ...   \n",
      "973857  8.351594e-06  4.187720e-07  0.000034  3.039982e-05  0.000018  ...   \n",
      "\n",
      "                E122          E123      E124          E125          E126  \\\n",
      "973853  4.278691e-06  4.844659e-06  0.000001 -7.152085e-07  3.723067e-07   \n",
      "973854  8.760097e-06  5.648323e-06  0.000005  1.763559e-06 -3.281198e-06   \n",
      "973855  8.460772e-06 -9.349405e-07  0.000001  1.542162e-06  5.759303e-06   \n",
      "973856 -5.704669e-07 -1.112272e-05 -0.000007  3.166437e-06  3.739454e-06   \n",
      "973857  2.535939e-05  2.448339e-05  0.000023  1.826776e-05  1.559515e-05   \n",
      "\n",
      "            E127      E128     time  label  subject  \n",
      "973853  0.000003  0.000003    9.992      1     0202  \n",
      "973854 -0.000010 -0.000007  328.244      1     0202  \n",
      "973855  0.000001  0.000002  462.908      1     0202  \n",
      "973856 -0.000010 -0.000005  174.968      1     0202  \n",
      "973857  0.000010  0.000018  696.368      1     0202  \n",
      "\n",
      "[5 rows x 131 columns]\n",
      "\n",
      "Distribution of Labels (0=Healthy, 1=Depressed):\n",
      "label\n",
      "1    772909\n",
      "0    200949\n",
      "Name: count, dtype: int64\n",
      "--- Verification Complete ---\n",
      "\n",
      "Found 72 missing values. Imputing with column means.\n",
      "\n",
      "--- Starting Hyperparameter Tuning ---\n",
      "Tuning KNN...\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best params for KNN: {'knn__n_neighbors': 9, 'pca__n_components': 45}\n",
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "[CV] END ...........knn__n_neighbors=9, pca__n_components=39; total time=  58.0s\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=40; total time=  57.8s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=42; total time=  59.2s\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=23; total time=  50.4s\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=21; total time=  49.3s\n",
      "[CV] END .....................max_depth=30, n_estimators=448; total time=27.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=34; total time=  56.1s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=27; total time=  51.8s\n",
      "[CV] END ...........knn__n_neighbors=9, pca__n_components=45; total time= 1.0min\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=30; total time=  52.7s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=41; total time=  54.9s\n",
      "[CV] END .....................max_depth=30, n_estimators=448; total time=27.3min\n",
      "[CV] END ...........knn__n_neighbors=9, pca__n_components=39; total time=  57.7s\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=40; total time=  57.1s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=42; total time=  58.6s\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=23; total time=  50.0s\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=21; total time=  49.3s\n",
      "[CV] END .....................max_depth=30, n_estimators=448; total time=27.4min\n",
      "[CV] END ...........knn__n_neighbors=7, pca__n_components=34; total time=  56.1s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=27; total time=  52.6s\n",
      "[CV] END ...........knn__n_neighbors=9, pca__n_components=45; total time=  59.8s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=30; total time=  53.0s\n",
      "[CV] END ...........knn__n_neighbors=5, pca__n_components=41; total time=  55.3s\n",
      "[CV] END .....................max_depth=30, n_estimators=206; total time=12.7min\n",
      "[CV] END .....................max_depth=30, n_estimators=206; total time=12.6min\n",
      "[CV] END .....................max_depth=30, n_estimators=206; total time=12.5min\n",
      "[CV] END .....................max_depth=10, n_estimators=202; total time= 5.6min\n",
      "[CV] END .....................max_depth=10, n_estimators=202; total time= 5.6min\n",
      "[CV] END .....................max_depth=20, n_estimators=314; total time=15.1min\n",
      "[CV] END .....................max_depth=10, n_estimators=199; total time= 5.5min\n",
      "[CV] END .....................max_depth=10, n_estimators=199; total time= 5.4min\n",
      "[CV] END .....................max_depth=30, n_estimators=249; total time=15.1min\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, LeaveOneGroupOut, learning_curve, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- 1. SETUP AND DATA LOADING ---\n",
    "directory_path = '/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1'\n",
    "output_dir = '/kaggle/working'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "subject_labels = {\n",
    "    # --- Depressed (MDD) Patients - Label 1 ---\n",
    "    '0201': 1, '0202': 1, # Add all MDD subject ID prefixes here\n",
    "    \n",
    "    # --- Healthy Controls (HC) - Label 0 ---\n",
    "    '0203': 0, '0204': 0, # Add all HC subject ID prefixes here\n",
    "}\n",
    "\n",
    "print(\"Loading, sampling, and labeling data...\")\n",
    "data_frames = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    subject_prefix = filename[:4]\n",
    "    if subject_prefix in subject_labels:\n",
    "        data = pd.read_csv(os.path.join(directory_path, filename))\n",
    "        data['label'] = subject_labels[subject_prefix]\n",
    "        data['subject'] = subject_prefix\n",
    "        sample = data.sample(frac=0.1, random_state=42)  # CHANGED: Reduced frac to 0.1 from 0.25 to shrink data size overall\n",
    "        data_frames.append(sample)\n",
    "\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "num_subjects = len(combined_data['subject'].unique())\n",
    "print(f\"Loaded {combined_data.shape[0]} rows of data from {num_subjects} unique subjects.\")\n",
    "\n",
    "# --- NEW CODE BLOCK: VERIFY THE LABELED DATA ---\n",
    "print(\"\\n--- Verifying the Combined and Labeled DataFrame ---\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(combined_data.head())\n",
    "print(\"\\nLast 5 rows of the dataset:\")\n",
    "print(combined_data.tail())\n",
    "print(\"\\nDistribution of Labels (0=Healthy, 1=Depressed):\")\n",
    "print(combined_data['label'].value_counts())\n",
    "print(\"--- Verification Complete ---\\n\")\n",
    "# --- END OF NEW CODE BLOCK ---\n",
    "\n",
    "if 'time' in combined_data.columns: combined_data.drop(columns=['time'], inplace=True)\n",
    "\n",
    "# --- 2. FEATURE/TARGET DEFINITION AND PREPROCESSING ---\n",
    "y = combined_data['label']\n",
    "groups = combined_data['subject']\n",
    "X = combined_data.drop(columns=['label', 'subject'])\n",
    "\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"Found {X.isnull().sum().sum()} missing values. Imputing with column means.\")\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "# --- 3. HYPERPARAMETER TUNING ---\n",
    "print(\"\\n--- Starting Hyperparameter Tuning ---\")\n",
    "param_dist = {\n",
    "    'KNN': {'pca__n_components': randint(20, 50), 'knn__n_neighbors': randint(3, 10)},  # CHANGED: Narrowed n_neighbors range to 3-10 for faster tuning\n",
    "    'Random Forest': {'n_estimators': randint(100, 500), 'max_depth': [10, 20, 30, None]},\n",
    "    'XGBoost': {'n_estimators': randint(100, 500), 'learning_rate': uniform(0.01, 0.2), 'max_depth': randint(3, 10)},\n",
    "    'Linear SVM': {'base_estimator__C': uniform(0.1, 10)},\n",
    "    'Logistic Regression': {'C': uniform(0.1, 10)}\n",
    "}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "best_params = {}\n",
    "models_to_tune = ['KNN', 'Random Forest', 'XGBoost', 'Linear SVM', 'Logistic Regression']\n",
    "\n",
    "for name in models_to_tune:\n",
    "    print(f\"Tuning {name}...\")\n",
    "    if name == 'KNN':\n",
    "        model_instance = Pipeline([('pca', PCA()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n",
    "    else:\n",
    "        model_instance = {\n",
    "            'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1),\n",
    "            'Linear SVM': CalibratedClassifierCV(LinearSVC(random_state=42, dual=False, max_iter=3000)),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "        }[name]\n",
    "    \n",
    "    # CHANGED: Further subsample for tuning to 20% of full data to speed up, especially for KNN\n",
    "    X_tune, _, y_tune, _ = train_test_split(X_scaled, y, train_size=0.2, stratify=y, random_state=42)\n",
    "    X_train_tune, _, y_train_tune, _ = train_test_split(X_tune, y_tune, train_size=0.8, stratify=y_tune, random_state=42)  # 80% of 20% = 16% effective\n",
    "    \n",
    "    random_search = RandomizedSearchCV(estimator=model_instance, param_distributions=param_dist[name], \n",
    "                                       n_iter=10 if name == 'KNN' else 15,  # CHANGED: Reduced n_iter to 10 for KNN only\n",
    "                                       cv=2 if name == 'KNN' else 3,  # CHANGED: Reduced cv to 2 for KNN only\n",
    "                                       scoring='accuracy', n_jobs=-1, random_state=42, verbose=2)  # CHANGED: verbose=2 for progress\n",
    "    random_search.fit(X_train_tune, y_train_tune)\n",
    "    best_params[name] = random_search.best_params_\n",
    "    print(f\"Best params for {name}: {best_params[name]}\")\n",
    "\n",
    "# --- 4. DEFINE MODELS WITH TUNED PARAMETERS ---\n",
    "tuned_knn_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=best_params.get('KNN', {}).get('pca__n_components', 30))),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=best_params.get('KNN', {}).get('knn__n_neighbors', 5), n_jobs=-1))\n",
    "])\n",
    "\n",
    "tuned_models = {\n",
    "    'Tuned KNN (PCA)': tuned_knn_pipeline,\n",
    "    'Tuned Linear SVM': CalibratedClassifierCV(LinearSVC(C=best_params.get('Linear SVM', {}).get('base_estimator__C', 1.0), random_state=42, dual=False, max_iter=3000)),\n",
    "    'Tuned RF': RandomForestClassifier(random_state=42, n_jobs=-1, **best_params.get('Random Forest', {})),\n",
    "    'Tuned XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, **best_params.get('XGBoost', {})),\n",
    "    'Tuned LogReg': LogisticRegression(max_iter=1000, random_state=42, **best_params.get('Logistic Regression', {})),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'LDA': LinearDiscriminantAnalysis()\n",
    "}\n",
    "cmap = plt.colormaps.get('tab10')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(tuned_models))]\n",
    "\n",
    "# --- 5. GENERATE PLOTS FROM A SINGLE TRAIN-TEST SPLIT (for visualization) ---\n",
    "print(\"\\n--- Generating Reports, Confusion Matrices, and ROC Curves from a single split ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Classification Reports Figure (unchanged)\n",
    "fig_reports, axes_reports = plt.subplots(4, 2, figsize=(18, 24)); axes_reports = axes_reports.flatten()\n",
    "fig_reports.suptitle('Classification Reports for All Models (Raw EEG Data)', fontsize=24, y=1.0)\n",
    "if len(tuned_models) % 2 != 0: fig_reports.delaxes(axes_reports[-1])\n",
    "\n",
    "# NEW: Confusion Matrices Figure\n",
    "fig_cm, axes_cm = plt.subplots(4, 2, figsize=(18, 24)); axes_cm = axes_cm.flatten()\n",
    "fig_cm.suptitle('Confusion Matrices for All Models (Raw EEG Data)', fontsize=24, y=1.0)\n",
    "if len(tuned_models) % 2 != 0: fig_cm.delaxes(axes_cm[-1])\n",
    "\n",
    "# ROC Figure (unchanged)\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(12, 10))\n",
    "ax_roc.set_title('ROC Curves for Depression Classification (Raw EEG Data)', fontsize=16)\n",
    "\n",
    "for i, (model_name, model) in enumerate(tuned_models.items()):\n",
    "    print(f\"Fitting {model_name} for plotting...\")\n",
    "    pipeline_plot = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    pipeline_plot.fit(X_train, y_train)\n",
    "    y_pred = pipeline_plot.predict(X_test)\n",
    "    \n",
    "    # Classification Report\n",
    "    report_text = classification_report(y_test, y_pred, target_names=['Healthy', 'Depressed'])\n",
    "    axes_reports[i].axis('off'); axes_reports[i].set_title(model_name, fontsize=16, pad=20)\n",
    "    axes_reports[i].text(0.01, 0.95, report_text, family='monospace', fontsize=14, va='top')\n",
    "    \n",
    "    # NEW: Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes_cm[i], cbar=False)\n",
    "    axes_cm[i].set_title(model_name, fontsize=16)\n",
    "    axes_cm[i].set_xlabel('Predicted'); axes_cm[i].set_ylabel('True')\n",
    "    axes_cm[i].set_xticklabels(['Healthy', 'Depressed']); axes_cm[i].set_yticklabels(['Healthy', 'Depressed'])\n",
    "    \n",
    "    # ROC Curve\n",
    "    if hasattr(pipeline_plot, \"predict_proba\"):\n",
    "        y_proba = pipeline_plot.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        ax_roc.plot(fpr, tpr, color=colors[i], lw=2.5, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Save Reports\n",
    "fig_reports.tight_layout(pad=3.0)\n",
    "fig_reports.savefig(os.path.join(output_dir, \"all_classification_reports_raw_csv.png\"))\n",
    "plt.close(fig_reports)\n",
    "print(f\"Saved consolidated classification reports to: {os.path.join(output_dir, 'all_classification_reports_raw_csv.png')}\")\n",
    "\n",
    "# Save Confusion Matrices\n",
    "fig_cm.tight_layout(pad=3.0)\n",
    "fig_cm.savefig(os.path.join(output_dir, \"all_confusion_matrices_raw_csv.png\"))\n",
    "plt.close(fig_cm)\n",
    "print(f\"Saved consolidated confusion matrices to: {os.path.join(output_dir, 'all_confusion_matrices_raw_csv.png')}\")\n",
    "\n",
    "# Save ROC\n",
    "ax_roc.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Guess')\n",
    "ax_roc.legend(loc='lower right', fontsize=12)\n",
    "ax_roc.grid(alpha=0.5)\n",
    "fig_roc.savefig(os.path.join(output_dir, \"all_roc_curves_raw_csv.png\"))\n",
    "plt.close(fig_roc)\n",
    "print(f\"Saved consolidated ROC curves to: {os.path.join(output_dir, 'all_roc_curves_raw_csv.png')}\")\n",
    "\n",
    "# --- 6. LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION ---\n",
    "print(\"\\n--- Performing Leave-One-Subject-Out Cross-Validation ---\")\n",
    "logo = LeaveOneGroupOut()\n",
    "cv_scores = {}\n",
    "for model_name, model in tuned_models.items():\n",
    "    print(f\"Evaluating {model_name} with LOSO-CV...\")\n",
    "    pipeline_cv = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    scores = cross_val_score(pipeline_cv, X, y, cv=logo, groups=groups, scoring='accuracy', n_jobs=-1)\n",
    "    cv_scores[model_name] = scores\n",
    "    print(f\"{model_name} Mean Accuracy: {np.mean(scores):.3f} Â± {np.std(scores):.3f}\")\n",
    "\n",
    "# --- 7. VISUALIZE LOSO-CV RESULTS ---\n",
    "fig_cv, ax_cv = plt.subplots(figsize=(14, 9))\n",
    "model_names = list(cv_scores.keys())\n",
    "mean_accuracies = [np.mean(s) for s in cv_scores.values()]\n",
    "std_devs = [np.std(s) for s in cv_scores.values()]\n",
    "ax_cv.bar(model_names, mean_accuracies, yerr=std_devs, capsize=5, color=colors, alpha=0.8)\n",
    "ax_cv.set_title(f'Leave-One-Subject-Out Cross-Validation Accuracy (n={num_subjects} subjects)', fontsize=16)\n",
    "ax_cv.set_ylabel('Mean Accuracy', fontsize=14); ax_cv.set_ylim(0, 1.05)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "for i, acc in enumerate(mean_accuracies): ax_cv.text(i, acc + 0.05, f\"{acc:.3f}\", ha='center')\n",
    "fig_cv.tight_layout()\n",
    "fig_cv.savefig(os.path.join(output_dir, \"loso_cv_accuracy_barchart.png\"))\n",
    "plt.close(fig_cv)\n",
    "print(f\"Saved LOSO-CV results chart to: {os.path.join(output_dir, 'loso_cv_accuracy_barchart.png')}\")\n",
    "\n",
    "# --- 8. GENERATE LEARNING CURVES ---\n",
    "print(\"\\n--- Generating Learning Curves ---\")\n",
    "fig_lc, axes_lc = plt.subplots(4, 2, figsize=(18, 24), sharey=True)\n",
    "if len(tuned_models) % 2 != 0: fig_lc.delaxes(axes_lc.flatten()[-1])\n",
    "axes_lc = axes_lc.flatten()\n",
    "fig_lc.suptitle('Learning Curves for All Models', fontsize=24, y=1.0)\n",
    "train_sizes_abs = np.linspace(0.1, 1.0, 7)\n",
    "\n",
    "for i, (model_name, model) in enumerate(tuned_models.items()):\n",
    "    print(f\"Generating learning curve for {model_name}...\")\n",
    "    pipeline_lc = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        pipeline_lc, X, y, cv=5, n_jobs=-1, train_sizes=train_sizes_abs, scoring='accuracy')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    \n",
    "    ax = axes_lc[i]\n",
    "    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    ax.set_title(model_name, fontsize=16)\n",
    "    ax.set_xlabel(\"Training examples\"); ax.set_ylabel(\"Accuracy\")\n",
    "    ax.grid(); ax.legend(loc=\"best\")\n",
    "\n",
    "fig_lc.tight_layout(pad=3.0)\n",
    "fig_lc.savefig(os.path.join(output_dir, \"all_learning_curves_raw_csv.png\"))\n",
    "plt.close(fig_lc)\n",
    "print(f\"Saved learning curves to: {os.path.join(output_dir, 'all_learning_curves_raw_csv.png')}\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14231560,
     "datasetId": 5468552,
     "sourceId": 13509254,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
