{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13509254,"sourceType":"datasetVersion","datasetId":5468552}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T09:01:56.366393Z","iopub.execute_input":"2025-11-24T09:01:56.366725Z","iopub.status.idle":"2025-11-24T09:01:57.087629Z","shell.execute_reply.started":"2025-11-24T09:01:56.366702Z","shell.execute_reply":"2025-11-24T09:01:57.087029Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010030rest 20160324 1054..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020025rest 20150713 1519..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010013rest 20150703 1333..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020016rest 20150701 1040..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020015_rest 20150630 1527.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010022restnew 20150724 14.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020027rest 20150713 1049..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010008_rest 20150619 1653.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010011rest 20150625 1516..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020029rest 20150715 1316..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020023restnew 20150709 10.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010012rest 20150626 1026..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030020_rest 20151230 1416.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010018rest 20150716 1237..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010002rest 20150416 1017..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020010rest 20150625 1224..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010016rest 20150710 1220..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020008rest 20150624 1711..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010024rest 20150814 1504..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030004_rest 20151026 1930.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020020rest 20150703 1754..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020018rest 20150702 1651..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010010rest 20150624 1447..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030002rest_new 20151022 1.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010033rest 20160331 1239..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020019rest 20150703 1036..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010005rest 20150507 0907..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030007_rest 20151103 2032.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030017_rest 20151208 1329.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030018_rest 20151208 1443.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020013rest 20150629 1607..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010026rest 20160311 1421..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030006_rest 20151103 1725.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010023rest 20150729 1929..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020021rest 20150707 1720..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020026_rest 20150714 1413.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030019_rest 20151230 1314.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030005rest 20151026 2103..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010015rest 20150709 1456..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030009_rest 20151105 1113.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010006rest 20150528 0928..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030003_rest 20151022 1155.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010004rest 20150427 1335..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020022rest 20150707 1452..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030021rest 20160105 1141..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010036_rest 20160408 1418.csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010034rest 20160407 0938..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010028rest 20160317 1538..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02030014rest 20151117 1441..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02010019rest 20150716 1440..csv\n/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2/02020014_rest 20150630 1023.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030017erp 20151208 1351-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010002erp 20150416 1131-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010011erp 20150625 1545-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010024erp 20150814 1523-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010018erp 20150716 1310-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010010erp 20150624 1508-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020025_erp 20150713 1541-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010030erp 20160324 0915-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030021erp 20160105 1204-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020020_erp 20150703 1810-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020027_erp 20150713 1116-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030020_erp 20151230 1443-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010015erp 20150709 1534-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010013erp 20150703 1353-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010023erp 20150729 1955-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030009erp 20151105 1207-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010025erp 20160311 1225-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010021erp 20150805 1818-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020016_erp 20150701 1054-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010034erp 20160407 0959-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020019_erp 20150703 1052-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030005erp 20151026 2116-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020013_erp 20150629 1622-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020029_erp 20150715 1401-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010019erp 20150716 1544-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020026_erp 20150714 1428-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010033erp 20160331 1307-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010005erp 20150507 0938-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010006erp 20150528 1007-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030019_erp 20151230 1331-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030006 20151103 1801-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010004erp 20141219 1602-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020010_erp 20150625 1244-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030007 20151103 2129-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030014erp 20151117 1419-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02030002erp_new 20151022 14-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020015_erp 20150630 1547-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020021_erp 20150707 1751-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010012erp 20150626 1059-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010028erp 20160317 1554-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020022_erp 20150707 1515-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020018_erp 20150702 1721-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020023_erpnew 20150709 110-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010026erp 20160311 1441-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010016erp 20150710 1329-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010008_erp(n) 20150619 1709-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020008_erp 20150624 1740-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010036erp 20160408 1452-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02010022erp 20150724 1457-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/raw-csv-actual/raw-csv-actual/csv_from_raw1/02020014_erp 20150630 1052-preprocessed.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/18.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/20.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/07.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/24.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/11.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/17.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/16.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/19.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/26.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/28.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/13.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/23.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/14.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/22.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/06.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/05.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/12.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/09.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/04.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/01.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/03.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/15.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/all_audio_features.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/10.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/08.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/25.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/29.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/21.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/27.csv\n/kaggle/input/preprocessed-raw-mat-csv/audio_features_csv_2/02.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Setup, load & preprocessing, save splits**","metadata":{}},{"cell_type":"code","source":"# CELL 1: Setup + Load + Preprocess + Save splits\nimport os, re, math, json, warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import IncrementalPCA\n\n# CONFIG\nDATA_DIR    = '/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2'\nOUTPUT_DIR  = '/kaggle/working/dl_results'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nSAMPLE_FRAC      = 1.0        # set 0.1 for quick tests\nUSE_IPCA         = True\nIPCA_COMPONENTS  = 128\nIPCA_BATCH       = 5000\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# GPU memory growth (optional)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor g in gpus:\n    try:\n        tf.config.experimental.set_memory_growth(g, True)\n    except Exception:\n        pass\n\n# helpers\ndef detect_eeg_columns(columns):\n    regex = re.compile(r'^(?:EEG[_\\-\\s]?|E[_\\-\\s]?)(0*?)(\\d{1,3})$', flags=re.I)\n    found = {}\n    for c in columns:\n        m = regex.match(c.strip())\n        if m:\n            num = int(m.group(2))\n            if 1 <= num <= 128:\n                found[num] = c\n    if found:\n        return [found[i] for i in sorted(found.keys())]\n    # fallback\n    return [c for c in columns if re.match(r'^(E|EEG)\\d+', c, flags=re.I)]\n\ndef to_binary_label_series(s):\n    s = s.dropna()\n    if s.empty: return None\n    s_num = pd.to_numeric(s, errors='coerce')\n    if s_num.notna().all():\n        uniq = set(np.unique(s_num))\n        if uniq.issubset({0,1}): return s_num.astype(int)\n        if uniq.issubset({1,2}): return s_num.map({1:0,2:1}).astype(int)\n        med = float(s_num.median()); return (s_num > med).astype(int)\n    s_str = s.astype(str)\n    unique_vals = s_str.unique()\n    if len(unique_vals) == 1: return s_str.map({unique_vals[0]:0}).astype(int)\n    if len(unique_vals) == 2:\n        le = LabelEncoder().fit(unique_vals)\n        return pd.Series(le.transform(s_str), index=s_str.index).astype(int)\n    mode_val = s_str.mode().iat[0]; return (s_str != mode_val).astype(int)\n\n# 1) Read CSVs\ncsvs = sorted([f for f in os.listdir(DATA_DIR) if f.endswith('.csv')])\nif len(csvs)==0:\n    raise RuntimeError(\"No CSV files in DATA_DIR\")\nprint(\"Found\", len(csvs), \"CSV files.\")\n\nparts = []\nfor fn in csvs:\n    path = os.path.join(DATA_DIR, fn)\n    df = pd.read_csv(path, engine='python')\n    if SAMPLE_FRAC is not None and 0 < SAMPLE_FRAC < 1.0:\n        df = df.sample(frac=SAMPLE_FRAC, random_state=SEED)\n    df['__source_file'] = os.path.splitext(fn)[0]\n    parts.append(df)\ncombined = pd.concat(parts, ignore_index=True)\nprint(\"Combined shape:\", combined.shape)\n\n# 2) label detection (prefer epoch, label, condition)\nlabel_cols_try = ['epoch','label','condition','cond','target']\nlabel_series = None\nfor c in label_cols_try:\n    if c in combined.columns:\n        s = to_binary_label_series(combined[c])\n        if s is not None:\n            label_series = pd.Series(index=combined.index, dtype=int)\n            label_series.loc[combined[c].dropna().index] = s\n            label_series = label_series.fillna(0).astype(int)\n            print(\"Using\", c, \"as labels.\")\n            break\nif label_series is None:\n    # fallback search\n    for c in combined.columns:\n        if c.startswith('__'): continue\n        s = to_binary_label_series(combined[c])\n        if s is not None:\n            label_series = pd.Series(index=combined.index, dtype=int)\n            label_series.loc[combined[c].dropna().index] = s\n            label_series = label_series.fillna(0).astype(int)\n            print(\"Fallback using\", c, \"as labels.\")\n            break\nif label_series is None:\n    raise RuntimeError(\"No suitable label column found. Ensure 'epoch'/'label' exists.\")\n\nprint(\"Label distribution:\", label_series.value_counts().to_dict())\nif label_series.nunique() <= 1:\n    print(\"Detected single class after mapping â€” abort and inspect label columns.\")\n    raise RuntimeError(\"Single-class dataset. Fix labels.\")\n\ncombined['__label'] = label_series.astype(int)\n\n# 3) Detect EEG columns & form feature matrix\neeg_cols = detect_eeg_columns(combined.columns)\nif not eeg_cols:\n    raise RuntimeError(\"No EEG columns detected; check column names.\")\nprint(\"Detected EEG columns:\", len(eeg_cols))\n# drop known metadata columns\ndrop_cols = {'time','condition','label','epoch','__source_file','__label'}\nfeature_cols = [c for c in eeg_cols if c not in drop_cols]\nif len(feature_cols) == 0:\n    raise RuntimeError(\"No feature columns after filtering.\")\nX_full = combined[feature_cols].to_numpy(dtype=np.float32)\ny = combined['__label'].to_numpy(dtype=np.int32)\nprint(\"X_full shape:\", X_full.shape, \"y shape:\", y.shape)\n\n# impute NaNs\nif np.isnan(X_full).any():\n    col_means = np.nanmean(X_full, axis=0)\n    inds = np.where(np.isnan(X_full)); X_full[inds] = np.take(col_means, inds[1])\n    print(\"Imputed NaNs.\")\n\n# 4) Optional IncrementalPCA\nif USE_IPCA and IPCA_COMPONENTS is not None and 0 < IPCA_COMPONENTS < X_full.shape[1]:\n    print(\"Running IncrementalPCA...\")\n    ipca = IncrementalPCA(n_components=IPCA_COMPONENTS)\n    n = X_full.shape[0]; bs = IPCA_BATCH\n    for i in range(0, n, bs):\n        ipca.partial_fit(X_full[i:i+bs])\n    X_reduced = np.empty((n, IPCA_COMPONENTS), dtype=np.float32)\n    for i in range(0, n, bs):\n        X_reduced[i:i+bs] = ipca.transform(X_full[i:i+bs]).astype(np.float32)\n    X = X_reduced\nelse:\n    X = X_full\nprint(\"Post-PCA shape:\", X.shape)\n\n# 5) scale and split (save splits for model cells)\nscaler = StandardScaler()\nX = scaler.fit_transform(X).astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n# persist splits so model cells can load them\nnp.savez_compressed(os.path.join(OUTPUT_DIR, 'data_split.npz'),\n                    X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\nprint(\"Saved data_split.npz to\", OUTPUT_DIR)\n# create empty models_results.json if not exists\nres_path = os.path.join(OUTPUT_DIR, 'models_results.json')\nif not os.path.exists(res_path):\n    with open(res_path,'w') as f: json.dump([], f)\nprint(\"Cell 1 done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:37:59.277972Z","iopub.execute_input":"2025-11-23T19:37:59.278326Z","iopub.status.idle":"2025-11-23T19:49:21.550563Z","shell.execute_reply.started":"2025-11-23T19:37:59.278306Z","shell.execute_reply":"2025-11-23T19:49:21.549782Z"}},"outputs":[{"name":"stderr","text":"2025-11-23 19:38:04.292186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763926684.772100      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763926684.934232      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Found 51 CSV files.\nCombined shape: (3862388, 133)\nUsing epoch as labels.\nLabel distribution: {0: 1932951, 1: 1929437}\nDetected EEG columns: 128\nX_full shape: (3862388, 128) y shape: (3862388,)\nPost-PCA shape: (3862388, 128)\nSaved data_split.npz to /kaggle/working/dl_results\nCell 1 done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Utility functions**","metadata":{}},{"cell_type":"code","source":"# CELL 2: Utility functions for model cells (run once)\nimport os, json, numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score\n\nOUTPUT_DIR = '/kaggle/working/dl_results'\ndef load_data_splits():\n    p = os.path.join(OUTPUT_DIR, 'data_split.npz')\n    d = np.load(p)\n    return d['X_train'], d['X_test'], d['y_train'], d['y_test']\n\ndef save_model_result(res):\n    \"\"\"Append JSON-serializable result dict to models_results.json\"\"\"\n    p = os.path.join(OUTPUT_DIR, 'models_results.json')\n    lst = []\n    if os.path.exists(p):\n        with open(p,'r') as f:\n            try:\n                lst = json.load(f)\n            except Exception:\n                lst = []\n    lst.append(res)\n    with open(p,'w') as f:\n        json.dump(lst, f)\n\ndef make_result_dict(name, model, X_test, y_test, history=None):\n    # predict probabilities where possible\n    try:\n        probs = model.predict(X_test, verbose=0).ravel()\n    except Exception:\n        # if model expects 3D or 4D, let caller reshape X_test appropriately before calling make_result_dict\n        probs = model.predict(X_test, verbose=0).ravel()\n    preds = (probs >= 0.5).astype(int)\n    acc = float(accuracy_score(y_test, preds))\n    try:\n        roc_auc = float(roc_auc_score(y_test, probs))\n    except Exception:\n        roc_auc = None\n    rep = classification_report(y_test, preds)\n    cm = confusion_matrix(y_test, preds).tolist()\n    try:\n        fpr,tpr,_ = roc_curve(y_test, probs)\n        fpr = fpr.tolist(); tpr = tpr.tolist()\n    except Exception:\n        fpr,tpr = [], []\n    hist_dict = history.history if history is not None else {}\n    # convert numpy types in hist to lists\n    clean_hist = {k: (list(np.array(v).astype(float)) if hasattr(v,'__iter__') else v) for k,v in hist_dict.items()}\n    res = {\n        'name': name,\n        'accuracy': acc,\n        'roc_auc': roc_auc,\n        'class_report': rep,\n        'conf_mat': cm,\n        'fpr': fpr,\n        'tpr': tpr,\n        'history': clean_hist\n    }\n    return res\n\nprint(\"Cell 2 loaded utilities.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:13:59.296267Z","iopub.execute_input":"2025-11-24T14:13:59.296491Z","iopub.status.idle":"2025-11-24T14:14:00.642037Z","shell.execute_reply.started":"2025-11-24T14:13:59.296467Z","shell.execute_reply":"2025-11-24T14:14:00.641277Z"}},"outputs":[{"name":"stdout","text":"Cell 2 loaded utilities.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Statistical Analysis**","metadata":{}},{"cell_type":"code","source":"# CELL: Per-subject graphs -> group-level node betweenness & clustering comparisons\n# - Loads each subject CSV separately (memory-safe)\n# - Builds per-subject correlation graphs (proportional threshold by density)\n# - Computes node betweenness & clustering coef per subject\n# - Aggregates across groups, runs tests (t-test or Mann-Whitney), computes Cohen's d\n# - Saves CSVs + histograms + boxplots to OUTPUT_DIR\nimport os, math, time, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport networkx as nx\nimport scipy.stats as st\n\n# --------- CONFIG ---------\nDATA_DIR   = r\"/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2\"  # edit if needed\nOUTPUT_DIR = r\"/kaggle/working/dl_results/per_subject_graphs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Graph thresholding: 'proportional' keeps top fraction of edges; set density in (0,1)\nTHRESH_METHOD = \"proportional\"\nDENSITY_KEEP  = 0.20   # keep top 20% absolute correlations as edges\nMIN_TIMESERIES_LEN = 5  # skip files with fewer samples\nLABEL_COL_CANDIDATES = ['epoch','label','condition','cond','target','y']  # try in order\n\nprint(\"OUTPUT_DIR:\", OUTPUT_DIR)\nstart_all = time.time()\n\n# ------- helpers -------\ndef detect_eeg_columns(cols):\n    import re\n    regex = re.compile(r'^(?:EEG[_\\-\\s]?|E[_\\-\\s]?)(0*?)(\\d{1,3})$', flags=re.I)\n    found = {}\n    for c in cols:\n        m = regex.match(str(c).strip())\n        if m:\n            num = int(m.group(2))\n            if 1 <= num <= 128:\n                found[num] = c\n    if found:\n        return [found[i] for i in sorted(found.keys())]\n    # fallback: pick numeric columns except obvious metadata\n    cand = [c for c in cols if isinstance(c, str) and c.lower() not in ('time','condition')]\n    return cand\n\ndef to_binary_label_series(s):\n    s = s.dropna()\n    if s.empty: return None\n    s_num = pd.to_numeric(s, errors='coerce')\n    if s_num.notna().all():\n        uniq = set(np.unique(s_num))\n        if uniq.issubset({0,1}): return s_num.astype(int)\n        if uniq.issubset({1,2}): return s_num.map({1:0,2:1}).astype(int)\n        med = float(s_num.median()); return (s_num > med).astype(int)\n    s_str = s.astype(str)\n    unique_vals = s_str.unique()\n    if len(unique_vals) == 1: return s_str.map({unique_vals[0]:0}).astype(int)\n    if len(unique_vals) == 2:\n        # deterministic mapping by sort\n        mapping = {unique_vals[0]:0, unique_vals[1]:1}\n        return s_str.map(mapping).astype(int)\n    mode_val = s_str.mode().iat[0]; return (s_str != mode_val).astype(int)\n\ndef threshold_proportional(abs_corr, keep_density):\n    # abs_corr: square matrix, diag = 1\n    n = abs_corr.shape[0]\n    iu = np.triu_indices(n, k=1)\n    vals = abs_corr[iu]\n    if vals.size == 0:\n        return np.zeros_like(abs_corr, dtype=int)\n    # determine cutoff so that proportion of edges kept approximates keep_density\n    k = int(np.floor(keep_density * (n*(n-1)/2)))\n    if k <= 0:\n        thr = 1.01  # keep none\n    else:\n        thr = np.sort(vals)[-k] if k < len(vals) else vals.min()\n    A = (abs_corr >= thr).astype(int)\n    np.fill_diagonal(A, 0)\n    return A\n\ndef cohen_d(x, y):\n    x = np.asarray(x); y = np.asarray(y)\n    nx_, ny_ = len(x), len(y)\n    if nx_ < 2 or ny_ < 2:\n        return np.nan\n    sx = x.std(ddof=1); sy = y.std(ddof=1)\n    if np.isnan(sx) or np.isnan(sy): return np.nan\n    pooled = np.sqrt(((nx_-1)*sx*sx + (ny_-1)*sy*sy) / (nx_+ny_-2))\n    if pooled == 0:\n        return np.nan\n    return (x.mean() - y.mean()) / pooled\n\n# ------- iterate over files (per-subject) -------\nfiles = sorted([f for f in os.listdir(DATA_DIR) if f.lower().endswith('.csv')])\nif len(files) == 0:\n    raise RuntimeError(\"No CSVs found in DATA_DIR\")\n\nrows = []   # per-subject summary rows\nnode_rows = []  # per-subject per-node metrics\n\nfor fn in files:\n    fp = os.path.join(DATA_DIR, fn)\n    try:\n        df = pd.read_csv(fp, engine='python')\n    except Exception as e:\n        print(\"skip (read fail):\", fn, e); continue\n\n    # detect label for this file\n    label_series = None; label_found = None\n    for c in LABEL_COL_CANDIDATES:\n        if c in df.columns:\n            s = to_binary_label_series(df[c])\n            if s is not None:\n                label_series = s\n                label_found = c\n                break\n    # fallback: if no label in-file, try filename mapping (source-level); assume filename contains class 0/1?\n    if label_series is None:\n        # default: no label -> skip (we need group membership)\n        print(\"skip (no label):\", fn); continue\n\n    # detect EEG/feature columns\n    eeg_cols = detect_eeg_columns(df.columns)\n    # remove metadata columns often present\n    eeg_cols = [c for c in eeg_cols if c not in ('time','condition','epoch','label')]\n    if len(eeg_cols) < 2:\n        # try numeric columns except label\n        cand = [c for c in df.columns if c not in (label_found, 'time','condition') and pd.api.types.is_numeric_dtype(df[c])]\n        eeg_cols = cand\n    if len(eeg_cols) < 2:\n        print(\"skip (not enough features):\", fn); continue\n\n    # prepare timeseries: shape (T, n_features)\n    data = df[eeg_cols].apply(pd.to_numeric, errors='coerce').values\n    # drop rows with NaNs\n    mask = ~np.isnan(data).any(axis=1)\n    data = data[mask]\n    if data.shape[0] < MIN_TIMESERIES_LEN:\n        print(\"skip (short timeseries):\", fn); continue\n\n    # compute feature x feature correlation across time (pearson)\n    # shape (n_features, n_features)\n    try:\n        C = np.corrcoef(data, rowvar=False)\n        C = np.nan_to_num(C, nan=0.0)\n    except Exception as e:\n        print(\"corr fail:\", fn, e); continue\n\n    absC = np.abs(C)\n\n    # threshold -> adjacency\n    if THRESH_METHOD == \"proportional\":\n        A = threshold_proportional(absC, DENSITY_KEEP)\n    else:\n        # fixed threshold fallback\n        thr = float(THRESH_METHOD)\n        A = (absC >= thr).astype(int); np.fill_diagonal(A,0)\n\n    # build graph\n    G = nx.from_numpy_array(A)\n    # compute node-level metrics\n    if G.number_of_nodes() == 0:\n        print(\"empty graph:\", fn); continue\n\n    # betweenness centrality\n    try:\n        bc = nx.betweenness_centrality(G, normalized=True)\n    except Exception:\n        # fallback approximate for large graphs\n        bc = nx.betweenness_centrality(G, normalized=True)\n    clust = nx.clustering(G)   # local clustering coefficient (unweighted)\n    degree = dict(G.degree())\n\n    # aggregate metrics (per-node)\n    for node in range(len(eeg_cols)):\n        node_rows.append({\n            'subject': fn,\n            'label': int(label_series.mode().iat[0]) if hasattr(label_series, 'mode') else int(label_series.iloc[0]),\n            'node_idx': int(node),\n            'feature_name': eeg_cols[node],\n            'betweenness': float(bc.get(node, 0.0)),\n            'clustering': float(clust.get(node, 0.0)),\n            'degree': int(degree.get(node, 0))\n        })\n\n    # subject-level summary (mean metrics across nodes)\n    mean_bc = np.mean(list(bc.values())) if len(bc)>0 else 0.0\n    mean_cl = np.mean(list(clust.values())) if len(clust)>0 else 0.0\n    rows.append({\n        'subject': fn,\n        'label': int(label_series.mode().iat[0]) if hasattr(label_series, 'mode') else int(label_series.iloc[0]),\n        'n_nodes': len(eeg_cols),\n        'n_time': data.shape[0],\n        'mean_betweenness': float(mean_bc),\n        'mean_clustering': float(mean_cl),\n        'density_used': float(A.sum() / (len(eeg_cols)*(len(eeg_cols)-1)))\n    })\n\n# ----- save raw per-node + per-subject tables -----\nnode_df = pd.DataFrame(node_rows)\nsubj_df = pd.DataFrame(rows)\nnode_df.to_csv(os.path.join(OUTPUT_DIR, \"per_subject_node_metrics.csv\"), index=False)\nsubj_df.to_csv(os.path.join(OUTPUT_DIR, \"per_subject_summary.csv\"), index=False)\nprint(\"Saved per-subject CSVs:\", os.path.join(OUTPUT_DIR, \"per_subject_node_metrics.csv\"))\n\n# -------- group-level comparisons --------\nif subj_df.shape[0] == 0:\n    raise RuntimeError(\"No subjects processed. Check DATA_DIR and label presence in files.\")\n\n# hist / box: subject-level mean metrics by group\ngroups = subj_df.groupby('label')\nlabels_present = sorted(subj_df['label'].unique())\n\n# boxplot: mean betweenness per subject\nplt.figure(figsize=(6,4))\nsns.boxplot(x='label', y='mean_betweenness', data=subj_df)\nplt.title(\"Subject mean node betweenness by group\")\nplt.xlabel(\"Label\"); plt.ylabel(\"Mean betweenness\")\nplt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, \"box_mean_betweenness_by_group.png\"), dpi=150); plt.close()\n\n# histogram overlay\nplt.figure(figsize=(6,4))\nfor lab in labels_present:\n    sns.histplot(subj_df.loc[subj_df['label']==lab, 'mean_betweenness'], label=f\"group {lab}\", kde=True, stat='density', bins=30)\nplt.legend(); plt.title(\"Histogram: subject mean betweenness\"); plt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"hist_mean_betweenness_by_group.png\"), dpi=150); plt.close()\n\n# clustering box + hist\nplt.figure(figsize=(6,4))\nsns.boxplot(x='label', y='mean_clustering', data=subj_df)\nplt.title(\"Subject mean clustering by group\")\nplt.xlabel(\"Label\"); plt.ylabel(\"Mean clustering\")\nplt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, \"box_mean_clustering_by_group.png\"), dpi=150); plt.close()\n\nplt.figure(figsize=(6,4))\nfor lab in labels_present:\n    sns.histplot(subj_df.loc[subj_df['label']==lab, 'mean_clustering'], label=f\"group {lab}\", kde=True, stat='density', bins=30)\nplt.legend(); plt.title(\"Histogram: subject mean clustering\"); plt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"hist_mean_clustering_by_group.png\"), dpi=150); plt.close()\n\n# --------- statistical tests: per-subject means ----------\nstats_rows = []\nif len(labels_present) == 2:\n    a = subj_df.loc[subj_df['label']==labels_present[0]]\n    b = subj_df.loc[subj_df['label']==labels_present[1]]\n    for metric in ['mean_betweenness','mean_clustering']:\n        xa = a[metric].values\n        xb = b[metric].values\n        # choose test depending on normality (Shapiro if n<=5000; else assume approx normal)\n        use_t = True\n        try:\n            if len(xa) >= 3 and len(xb) >= 3:\n                pa = st.shapiro(xa).pvalue\n                pb = st.shapiro(xb).pvalue\n                use_t = (pa>0.05 and pb>0.05)\n        except Exception:\n            use_t = False\n        if use_t:\n            stat, p = st.ttest_ind(xa, xb, equal_var=False)\n            test = 'ttest'\n        else:\n            stat, p = st.mannwhitneyu(xa, xb, alternative='two-sided')\n            test = 'mannwhitneyu'\n        d = cohen_d(xa, xb)\n        stats_rows.append({'metric': metric, 'test': test, 'stat': float(stat), 'pval': float(p), 'cohen_d': float(d)})\nelse:\n    # multi-group: Kruskal-Wallis on subject means\n    for metric in ['mean_betweenness','mean_clustering']:\n        groups_vals = [g[metric].values for _, g in subj_df.groupby('label')]\n        try:\n            stat, p = st.kruskal(*groups_vals)\n            test = 'kruskal'\n        except Exception:\n            stat, p, test = np.nan, np.nan, 'na'\n        stats_rows.append({'metric': metric, 'test': test, 'stat': float(stat) if not np.isnan(stat) else None, 'pval': float(p) if not np.isnan(p) else None, 'cohen_d': None})\n\nstats_df = pd.DataFrame(stats_rows)\nstats_df.to_csv(os.path.join(OUTPUT_DIR, \"subject_level_stats.csv\"), index=False)\nprint(\"Saved subject-level stats:\", os.path.join(OUTPUT_DIR, \"subject_level_stats.csv\"))\n\n# -------- node-level testing (per-node across subjects) --------\n# pivot node_df so each subject provides a value per node; then test node-wise between groups\nif node_df.shape[0] > 0 and len(labels_present) == 2:\n    node_stats = []\n    nodes = sorted(node_df['node_idx'].unique())\n    for node in nodes:\n        subn = node_df[node_df['node_idx'] == node]\n        ga = subn[subn['label']==labels_present[0]]['betweenness'].values\n        gb = subn[subn['label']==labels_present[1]]['betweenness'].values\n        # require >=2 per group\n        if len(ga) < 2 or len(gb) < 2:\n            continue\n        # test\n        use_t = True\n        try:\n            pa = st.shapiro(ga).pvalue if len(ga)>=3 else 1.0\n            pb = st.shapiro(gb).pvalue if len(gb)>=3 else 1.0\n            use_t = (pa>0.05 and pb>0.05)\n        except Exception:\n            use_t = False\n        if use_t:\n            stat, p = st.ttest_ind(ga, gb, equal_var=False)\n            test = 'ttest'\n        else:\n            stat, p = st.mannwhitneyu(ga, gb, alternative='two-sided')\n            test = 'mannwhitneyu'\n        d = cohen_d(ga, gb)\n        node_stats.append({'node_idx': int(node), 'feature_name': subn['feature_name'].iloc[0], 'test': test, 'stat': float(stat), 'pval': float(p), 'cohen_d': float(d)})\n    node_stats_df = pd.DataFrame(node_stats).sort_values('pval')\n    node_stats_df.to_csv(os.path.join(OUTPUT_DIR, \"node_level_stats_betweenness.csv\"), index=False)\n    print(\"Saved node-level stats (betweenness):\", os.path.join(OUTPUT_DIR, \"node_level_stats_betweenness.csv\"))\n\n    # top nodes by effect (abs cohen d)\n    if not node_stats_df.empty:\n        top_nodes = node_stats_df.sort_values('cohen_d', key=lambda s: np.abs(s), ascending=False).head(12)\n        # plot a small heatmap of cohen_d for top nodes\n        plt.figure(figsize=(6, max(2, len(top_nodes)*0.25)))\n        sns.barplot(x=np.abs(top_nodes['cohen_d']), y=top_nodes['feature_name'])\n        plt.title(\"Top nodes by |Cohen's d| (betweenness)\")\n        plt.xlabel(\"|Cohen's d|\"); plt.tight_layout()\n        plt.savefig(os.path.join(OUTPUT_DIR, \"top_nodes_cohen_d_betweenness.png\"), dpi=150); plt.close()\n\n# --------- final small summary JSON ----------\nsummary = {\n    'n_subjects': int(subj_df.shape[0]),\n    'n_files_scanned': len(files),\n    'labels_present': labels_present,\n    'density_keep': float(DENSITY_KEEP),\n    'thresh_method': THRESH_METHOD,\n    'generated': time.time()\n}\nwith open(os.path.join(OUTPUT_DIR, \"per_subject_graphs_summary.json\"), \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"Done. All outputs saved under:\", OUTPUT_DIR)\nprint(\"Elapsed (s):\", time.time() - start_all)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:55:14.554802Z","iopub.execute_input":"2025-11-23T19:55:14.556243Z","iopub.status.idle":"2025-11-23T20:04:26.373391Z","shell.execute_reply.started":"2025-11-23T19:55:14.556212Z","shell.execute_reply":"2025-11-23T20:04:26.372320Z"}},"outputs":[{"name":"stdout","text":"OUTPUT_DIR: /kaggle/working/dl_results/per_subject_graphs\nSaved per-subject CSVs: /kaggle/working/dl_results/per_subject_graphs/per_subject_node_metrics.csv\nSaved subject-level stats: /kaggle/working/dl_results/per_subject_graphs/subject_level_stats.csv\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/899993362.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m }\n\u001b[1;32m    336\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"per_subject_graphs_summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done. All outputs saved under:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"],"ename":"TypeError","evalue":"Object of type int64 is not JSON serializable","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# ==============================  \n# FIXED CELL: Per-subject Graphs  \n# JSON-SAFE + int64-SAFE  \n# ==============================\nimport os, math, time, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport networkx as nx\nimport scipy.stats as st\n\n# --------- CONFIG ---------\nDATA_DIR   = r\"/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2\"\nOUTPUT_DIR = r\"/kaggle/working/dl_results/per_subject_graphs_main\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nTHRESH_METHOD = \"proportional\"\nDENSITY_KEEP  = 0.20\nMIN_TIMESERIES_LEN = 5\nLABEL_COL_CANDIDATES = ['epoch','label','condition','cond','target','y']\n\nstart_all = time.time()\n\n# ---- Helpers (same as before, unchanged) ----\ndef detect_eeg_columns(cols):\n    import re\n    regex = re.compile(r'^(?:EEG[_\\-\\s]?|E[_\\-\\s]?)(0*?)(\\d{1,3})$', flags=re.I)\n    found = {}\n    for c in cols:\n        m = regex.match(str(c).strip())\n        if m:\n            num = int(m.group(2))\n            if 1 <= num <= 128:\n                found[num] = c\n    if found:\n        return [found[i] for i in sorted(found.keys())]\n    cand = [c for c in cols if isinstance(c, str) and c.lower() not in ('time','condition')]\n    return cand\n\ndef to_binary_label_series(s):\n    s = s.dropna()\n    if s.empty: return None\n    s_num = pd.to_numeric(s, errors='coerce')\n    if s_num.notna().all():\n        uniq = set(np.unique(s_num))\n        if uniq.issubset({0,1}): return s_num.astype(int)\n        if uniq.issubset({1,2}): return s_num.map({1:0,2:1}).astype(int)\n        med = float(s_num.median()); return (s_num > med).astype(int)\n    s_str = s.astype(str)\n    uniq = s_str.unique()\n    if len(uniq)==1:\n        return s_str.map({uniq[0]:0}).astype(int)\n    if len(uniq)==2:\n        return s_str.map({uniq[0]:0, uniq[1]:1}).astype(int)\n    mode = s_str.mode().iat[0]; return (s_str != mode).astype(int)\n\ndef threshold_proportional(abs_corr, keep_density):\n    n = abs_corr.shape[0]\n    iu = np.triu_indices(n, 1)\n    vals = abs_corr[iu]\n    if vals.size == 0:\n        return np.zeros_like(abs_corr, int)\n    k = int(np.floor(keep_density * (n*(n-1)/2)))\n    if k <= 0:\n        thr = 1.01\n    else:\n        thr = np.sort(vals)[-k] if k < len(vals) else vals.min()\n    A = (abs_corr >= thr).astype(int)\n    np.fill_diagonal(A, 0)\n    return A\n\ndef cohen_d(a, b):\n    a = np.asarray(a); b = np.asarray(b)\n    if len(a)<2 or len(b)<2: return np.nan\n    s1 = a.std(ddof=1); s2 = b.std(ddof=1)\n    if s1==0 and s2==0: return np.nan\n    pooled = np.sqrt(((len(a)-1)*s1*s1 + (len(b)-1)*s2*s2) / (len(a)+len(b)-2))\n    if pooled == 0: return np.nan\n    return (a.mean() - b.mean()) / pooled\n\n# ---------- MAIN LOOP ----------\nfiles = sorted([f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".csv\")])\n\nrows = []\nnode_rows = []\n\nfor fn in files:\n    fp = os.path.join(DATA_DIR, fn)\n    try:\n        df = pd.read_csv(fp, engine=\"python\")\n    except:\n        continue\n\n    # label\n    label_series = None\n    label_used = None\n    for c in LABEL_COL_CANDIDATES:\n        if c in df.columns:\n            tmp = to_binary_label_series(df[c])\n            if tmp is not None:\n                label_series = tmp\n                label_used = c\n                break\n    if label_series is None:\n        continue\n\n    # EEG columns\n    eeg_cols = detect_eeg_columns(df.columns)\n    eeg_cols = [c for c in eeg_cols if c not in ('time','condition','epoch','label')]\n    if len(eeg_cols) < 2:\n        continue\n\n    # data matrix\n    data = df[eeg_cols].apply(pd.to_numeric, errors='coerce').values\n    mask = ~np.isnan(data).any(axis=1)\n    data = data[mask]\n    if data.shape[0] < MIN_TIMESERIES_LEN:\n        continue\n\n    try:\n        C = np.corrcoef(data, rowvar=False)\n        C = np.nan_to_num(C)\n    except:\n        continue\n\n    absC = np.abs(C)\n\n    # adjacency\n    A = threshold_proportional(absC, DENSITY_KEEP)\n\n    G = nx.from_numpy_array(A)\n    if G.number_of_nodes() == 0:\n        continue\n\n    bc = nx.betweenness_centrality(G)\n    cl = nx.clustering(G)\n    deg = dict(G.degree())\n\n    subject_label = int(label_series.mode().iat[0])\n\n    # node rows\n    for i, col in enumerate(eeg_cols):\n        node_rows.append({\n            \"subject\": fn,\n            \"label\": subject_label,\n            \"node_idx\": int(i),\n            \"feature_name\": str(col),\n            \"betweenness\": float(bc[i]),\n            \"clustering\": float(cl[i]),\n            \"degree\": int(deg[i])\n        })\n\n    rows.append({\n        \"subject\": fn,\n        \"label\": subject_label,\n        \"n_nodes\": int(len(eeg_cols)),\n        \"n_time\": int(data.shape[0]),\n        \"mean_betweenness\": float(np.mean(list(bc.values()))),\n        \"mean_clustering\": float(np.mean(list(cl.values()))),\n        \"density_used\": float(A.sum() / (len(eeg_cols)*(len(eeg_cols)-1)))\n    })\n\nnode_df = pd.DataFrame(node_rows)\nsubj_df = pd.DataFrame(rows)\n\nnode_df.to_csv(os.path.join(OUTPUT_DIR, \"per_subject_node_metrics.csv\"), index=False)\nsubj_df.to_csv(os.path.join(OUTPUT_DIR, \"per_subject_summary.csv\"), index=False)\n\nprint(\"Saved per-subject tables.\")\n\n# ---------- GROUP COMPARE ----------\nlabels_present = sorted([int(x) for x in subj_df[\"label\"].unique()])\n\n# boxplots\nplt.figure(figsize=(6,4))\nsns.boxplot(x=\"label\", y=\"mean_betweenness\", data=subj_df)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"box_mean_betw.png\"))\nplt.close()\n\nplt.figure(figsize=(6,4))\nsns.boxplot(x=\"label\", y=\"mean_clustering\", data=subj_df)\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"box_mean_clust.png\"))\nplt.close()\n\n# histograms\nplt.figure(figsize=(6,4))\nfor lab in labels_present:\n    sns.histplot(\n        subj_df[subj_df[\"label\"]==lab][\"mean_betweenness\"],\n        kde=True, stat=\"density\", bins=30, label=f\"group {lab}\"\n    )\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"hist_mean_betw.png\"))\nplt.close()\n\nplt.figure(figsize=(6,4))\nfor lab in labels_present:\n    sns.histplot(\n        subj_df[subj_df[\"label\"]==lab][\"mean_clustering\"],\n        kde=True, stat=\"density\", bins=30, label=f\"group {lab}\"\n    )\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"hist_mean_clust.png\"))\nplt.close()\n\n# -------- stats: subject-level --------\nstats_rows = []\nif len(labels_present)==2:\n    g0 = subj_df[subj_df[\"label\"]==labels_present[0]]\n    g1 = subj_df[subj_df[\"label\"]==labels_present[1]]\n\n    for metric in [\"mean_betweenness\",\"mean_clustering\"]:\n        a = g0[metric].values\n        b = g1[metric].values\n        try:\n            p0 = st.shapiro(a).pvalue if len(a)>=3 else 1\n            p1 = st.shapiro(b).pvalue if len(b)>=3 else 1\n            use_t = (p0>0.05 and p1>0.05)\n        except:\n            use_t = False\n\n        if use_t:\n            stat, p = st.ttest_ind(a, b, equal_var=False)\n            test = \"ttest\"\n        else:\n            stat, p = st.mannwhitneyu(a, b)\n            test = \"mannwhitney\"\n\n        stats_rows.append({\n            \"metric\": metric,\n            \"test\": test,\n            \"stat\": float(stat),\n            \"pval\": float(p),\n            \"cohen_d\": float(cohen_d(a,b))\n        })\n\nstats_df = pd.DataFrame(stats_rows)\nstats_df.to_csv(os.path.join(OUTPUT_DIR, \"subject_level_stats.csv\"), index=False)\n\n# --------- JSON SUMMARY (FIXED) ----------\nsummary = {\n    \"n_subjects\": int(subj_df.shape[0]),\n    \"n_files_scanned\": int(len(files)),\n    \"labels_present\": [int(x) for x in labels_present],\n    \"density_keep\": float(DENSITY_KEEP),\n    \"thresh_method\": str(THRESH_METHOD),\n    \"generated_time\": float(time.time())\n}\n\nwith open(os.path.join(OUTPUT_DIR, \"per_subject_graphs_summary.json\"), \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"Done. JSON saved safely.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:09:05.484278Z","iopub.execute_input":"2025-11-23T20:09:05.484704Z","iopub.status.idle":"2025-11-23T20:17:12.420317Z","shell.execute_reply.started":"2025-11-23T20:09:05.484673Z","shell.execute_reply":"2025-11-23T20:17:12.419501Z"}},"outputs":[{"name":"stdout","text":"Saved per-subject tables.\nDone. JSON saved safely.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================\n# FIXED CELL â€” Per-Subject Graphs\n# Robust Labels + JSON-safe + Group Stats\n# ================================\n\nimport os, json, time, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# ------------ CONFIG ------------\nDATA_DIR   = r\"/kaggle/input/preprocessed-raw-mat-csv/mat-csv-actual/mat-csv-actual/csv_files_from_mat2\"\nOUTPUT_DIR = r\"/kaggle/working/dl_results/per_subject_graphs_main_2\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nTHRESH_METHOD = \"proportional\"\nDENSITY_KEEP  = 0.20\nMIN_TIMESERIES_LEN = 5\nLABEL_CANDIDATES = [\"label\",\"epoch\",\"condition\",\"cond\",\"y\",\"target\"]\n\nstart_all = time.time()\n\n# ===================== Helpers =====================\n\ndef detect_eeg_columns(cols):\n    import re\n    regex = re.compile(r'^(?:EEG|E)[ _-]*0*(\\d{1,3})$', flags=re.I)\n    idx = {}\n    for c in cols:\n        m = regex.match(str(c).strip())\n        if m:\n            n = int(m.group(1))\n            if 1 <= n <= 128:\n                idx[n] = c\n    if idx:\n        return [idx[i] for i in sorted(idx.keys())]\n\n    # fallback: pick numeric columns\n    return [c for c in cols if pd.api.types.is_numeric_dtype(cols[c])]\n\n\ndef to_binary_label_series(s):\n    s = s.dropna()\n    if s.empty: return None\n    s_num = pd.to_numeric(s, errors=\"coerce\")\n    if s_num.notna().all():\n        uniq = set(s_num.unique())\n        if uniq.issubset({0,1}): return s_num.astype(int)\n        if uniq.issubset({1,2}): return s_num.map({1:0,2:1}).astype(int)\n        med = float(s_num.median())\n        return (s_num > med).astype(int)\n\n    s_str = s.astype(str)\n    uniq = s_str.unique()\n    if len(uniq)==1: return s_str.map({uniq[0]:0}).astype(int)\n    if len(uniq)==2: return s_str.map({uniq[0]:0, uniq[1]:1}).astype(int)\n    mode = s_str.mode().iat[0]\n    return (s_str != mode).astype(int)\n\n\ndef threshold_proportional(abs_corr, keep_density):\n    n = abs_corr.shape[0]\n    iu = np.triu_indices(n,1)\n    vals = abs_corr[iu]\n    if vals.size == 0:\n        return np.zeros_like(abs_corr,int)\n    k = int(np.floor(keep_density*(n*(n-1)/2)))\n    if k <= 0:\n        thr = 1.01\n    else:\n        thr = np.sort(vals)[-k] if k < len(vals) else vals.min()\n    A = (abs_corr >= thr).astype(int)\n    np.fill_diagonal(A,0)\n    return A\n\n\ndef cohen_d(a, b):\n    a = np.asarray(a); b = np.asarray(b)\n    if len(a)<2 or len(b)<2: return np.nan\n    sa = a.std(ddof=1); sb = b.std(ddof=1)\n    pooled = np.sqrt(((len(a)-1)*sa*sa + (len(b)-1)*sb*sb)/(len(a)+len(b)-2))\n    if pooled == 0: return np.nan\n    return (a.mean() - b.mean()) / pooled\n\n\n# ===================== MAIN LOOP =====================\n\nfiles = sorted([f for f in os.listdir(DATA_DIR) if f.lower().endswith(\".csv\")])\nrows = []\nnode_rows = []\n\nprint(f\"Found {len(files)} CSV files.\")\n\nfor fn in files:\n    fp = os.path.join(DATA_DIR, fn)\n    try:\n        df = pd.read_csv(fp)\n    except:\n        print(f\"skip (read error): {fn}\")\n        continue\n\n    # ----- robust label detection -----\n    label_series = None\n    for col in LABEL_CANDIDATES:\n        if col in df.columns:\n            tmp = to_binary_label_series(df[col])\n            if tmp is not None:\n                label_series = tmp\n                break\n    if label_series is None:\n        print(f\"skip (no label found): {fn}\")\n        continue\n\n    uniq_vals = pd.Series(label_series.unique()).dropna().astype(int).tolist()\n    majority_thresh = 0.70\n\n    # Decide subject-level label\n    if len(uniq_vals) == 1:\n        subject_label = int(uniq_vals[0])\n\n    else:\n        mode_val = int(label_series.mode().iat[0])\n        mode_prop = float((label_series == mode_val).mean())\n\n        if mode_prop >= majority_thresh:\n            subject_label = mode_val\n            print(f\"warning: mixed labels in {fn}, using mode={mode_val}, prop={mode_prop:.2f}\")\n\n        else:\n            # ambiguous, fall back to mode but warn\n            subject_label = mode_val\n            print(f\"warning: {fn} very mixed labels (prop={mode_prop:.2f}) â€” using mode={mode_val} (may want subject-level mapping)\")\n\n    # ----- detect EEG columns -----\n    eeg_cols = detect_eeg_columns(df)\n    if len(eeg_cols) < 2:\n        print(f\"skip (no EEG cols): {fn}\")\n        continue\n\n    # ----- prepare data -----\n    data = df[eeg_cols].apply(pd.to_numeric, errors='coerce').values\n    mask = ~np.isnan(data).any(axis=1)\n    data = data[mask]\n    if data.shape[0] < MIN_TIMESERIES_LEN:\n        print(f\"skip (short timeseries): {fn}\")\n        continue\n\n    # ----- correlation -----\n    try:\n        C = np.corrcoef(data, rowvar=False)\n        C = np.nan_to_num(C)\n    except:\n        print(f\"corr fail: {fn}\")\n        continue\n\n    absC = np.abs(C)\n    A = threshold_proportional(absC, DENSITY_KEEP)\n\n    G = nx.from_numpy_array(A)\n    if G.number_of_nodes() == 0:\n        print(f\"empty graph: {fn}\")\n        continue\n\n    bc = nx.betweenness_centrality(G)\n    cl = nx.clustering(G)\n    dg = dict(G.degree())\n\n    # ===== per-node rows =====\n    for idx, col in enumerate(eeg_cols):\n        node_rows.append({\n            \"subject\": fn,\n            \"label\": int(subject_label),\n            \"node_idx\": int(idx),\n            \"feature\": str(col),\n            \"betweenness\": float(bc[idx]),\n            \"clustering\": float(cl[idx]),\n            \"degree\": int(dg[idx])\n        })\n\n    # ===== per-subject summary =====\n    rows.append({\n        \"subject\": fn,\n        \"label\": int(subject_label),\n        \"n_nodes\": int(len(eeg_cols)),\n        \"n_time\": int(data.shape[0]),\n        \"mean_betweenness\": float(np.mean(list(bc.values()))),\n        \"mean_clustering\": float(np.mean(list(cl.values()))),\n        \"density_used\": float(A.sum() / (len(eeg_cols)*(len(eeg_cols)-1)))\n    })\n\n\n# ===================== SAVE TABLES =====================\n\nnode_df = pd.DataFrame(node_rows)\nsubj_df = pd.DataFrame(rows)\n\nnode_df.to_csv(os.path.join(OUTPUT_DIR, \"per_subject_node_metrics.csv\"), index=False)\nsubj_df.to_csv(os.path.join(OUTPUT_DIR, \"per_subject_summary.csv\"), index=False)\n\nprint(f\"Saved: {OUTPUT_DIR}\")\n\n\n# ===================== GROUP COMPARISONS =====================\n\nsubj_df[\"label\"] = subj_df[\"label\"].astype(int)\nlabels_present = sorted(subj_df[\"label\"].unique().tolist())\n\nprint(\"Subject label counts:\", subj_df[\"label\"].value_counts().to_dict())\n\n# ----- Boxplots -----\nplt.figure(figsize=(6,4))\nsns.boxplot(x=\"label\", y=\"mean_betweenness\", data=subj_df)\nplt.title(\"Mean Betweenness by Group\")\nplt.savefig(os.path.join(OUTPUT_DIR, \"box_mean_betweenness.png\"))\nplt.close()\n\nplt.figure(figsize=(6,4))\nsns.boxplot(x=\"label\", y=\"mean_clustering\", data=subj_df)\nplt.title(\"Mean Clustering by Group\")\nplt.savefig(os.path.join(OUTPUT_DIR, \"box_mean_clustering.png\"))\nplt.close()\n\n# ----- Histograms -----\nplt.figure(figsize=(6,4))\nfor lab in labels_present:\n    sns.histplot(subj_df[subj_df[\"label\"]==lab][\"mean_betweenness\"],\n                 kde=True, stat=\"density\", bins=30, label=f\"group {lab}\")\nplt.legend()\nplt.title(\"Histogram: Mean Betweenness\")\nplt.savefig(os.path.join(OUTPUT_DIR, \"hist_mean_betweenness.png\"))\nplt.close()\n\nplt.figure(figsize=(6,4))\nfor lab in labels_present:\n    sns.histplot(subj_df[subj_df[\"label\"]==lab][\"mean_clustering\"],\n                 kde=True, stat=\"density\", bins=30, label=f\"group {lab}\")\nplt.legend()\nplt.title(\"Histogram: Mean Clustering\")\nplt.savefig(os.path.join(OUTPUT_DIR, \"hist_mean_clustering.png\"))\nplt.close()\n\n# ----- Stats -----\nstats_rows = []\nif len(labels_present)==2:\n    g0 = subj_df[subj_df['label']==labels_present[0]]\n    g1 = subj_df[subj_df['label']==labels_present[1]]\n\n    for metric in [\"mean_betweenness\",\"mean_clustering\"]:\n        a = g0[metric].values\n        b = g1[metric].values\n\n        try:\n            p0 = st.shapiro(a).pvalue if len(a)>=3 else 1\n            p1 = st.shapiro(b).pvalue if len(b)>=3 else 1\n            use_t = (p0>0.05 and p1>0.05)\n        except:\n            use_t = False\n\n        if use_t:\n            stat, p = st.ttest_ind(a, b, equal_var=False)\n            test = \"ttest\"\n        else:\n            stat, p = st.mannwhitneyu(a, b)\n            test = \"mannwhitney\"\n\n        stats_rows.append({\n            \"metric\": metric,\n            \"test\": test,\n            \"stat\": float(stat),\n            \"pval\": float(p),\n            \"cohen_d\": float(cohen_d(a,b))\n        })\n\nstats_df = pd.DataFrame(stats_rows)\nstats_df.to_csv(os.path.join(OUTPUT_DIR, \"subject_level_stats.csv\"), index=False)\n\n# ----- JSON SUMMARY -----\nsummary = {\n    \"n_subjects\": int(subj_df.shape[0]),\n    \"n_files_scanned\": int(len(files)),\n    \"labels_present\": [int(x) for x in labels_present],\n    \"thresh_method\": THRESH_METHOD,\n    \"density_keep\": float(DENSITY_KEEP),\n    \"generated\": float(time.time())\n}\n\nwith open(os.path.join(OUTPUT_DIR, \"summary.json\"), \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"DONE. Outputs saved in:\", OUTPUT_DIR)\nprint(\"Elapsed:\", time.time() - start_all, \"sec\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:26:55.105776Z","iopub.execute_input":"2025-11-23T20:26:55.106598Z","iopub.status.idle":"2025-11-23T20:28:57.380200Z","shell.execute_reply.started":"2025-11-23T20:26:55.106570Z","shell.execute_reply":"2025-11-23T20:28:57.379470Z"}},"outputs":[{"name":"stdout","text":"Found 51 CSV files.\nwarning: 02010002rest 20150416 1017..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010004rest 20150427 1335..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010005rest 20150507 0907..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010006rest 20150528 0928..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010008_rest 20150619 1653.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010010rest 20150624 1447..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010011rest 20150625 1516..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010012rest 20150626 1026..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010013rest 20150703 1333..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010015rest 20150709 1456..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010016rest 20150710 1220..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010018rest 20150716 1237..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010019rest 20150716 1440..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010022restnew 20150724 14.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010023rest 20150729 1929..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010024rest 20150814 1504..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010026rest 20160311 1421..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010028rest 20160317 1538..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010030rest 20160324 1054..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010033rest 20160331 1239..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010034rest 20160407 0938..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02010036_rest 20160408 1418.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020008rest 20150624 1711..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020010rest 20150625 1224..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020013rest 20150629 1607..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020014_rest 20150630 1023.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020015_rest 20150630 1527.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020016rest 20150701 1040..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020018rest 20150702 1651..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020019rest 20150703 1036..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020020rest 20150703 1754..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020021rest 20150707 1720..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020022rest 20150707 1452..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020023restnew 20150709 10.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020025rest 20150713 1519..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020026_rest 20150714 1413.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020027rest 20150713 1049..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02020029rest 20150715 1316..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030002rest_new 20151022 1.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030003_rest 20151022 1155.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030004_rest 20151026 1930.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030005rest 20151026 2103..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030006_rest 20151103 1725.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030007_rest 20151103 2032.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030009_rest 20151105 1113.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030014rest 20151117 1441..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030017_rest 20151208 1329.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030018_rest 20151208 1443.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030019_rest 20151230 1314.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030020_rest 20151230 1416.csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nwarning: 02030021rest 20160105 1141..csv very mixed labels (prop=0.50) â€” using mode=0 (may want subject-level mapping)\nSaved: /kaggle/working/dl_results/per_subject_graphs_main_2\nSubject label counts: {0: 51}\nDONE. Outputs saved in: /kaggle/working/dl_results/per_subject_graphs_main_2\nElapsed: 122.2440288066864 sec\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **GAN-Fixed**","metadata":{}},{"cell_type":"code","source":"# ================================\n# CELL 3 â€” Vanilla GAN (Kaggle-optimized, faster)\n# - saves ONLY best generator + best discriminator weights (.weights.h5)\n# - adaptive early-stopping for adversarial loop to speed up on T4\n# - no synthetic datasets saved, minimal PNGs + JSON\n# ================================\nimport os, time, gc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import backend as K\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n\n# -------- CONFIG --------\nOUTPUT_DIR = \"/kaggle/working/dl_results\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nLATENT_DIM = 100\nEPOCHS_GAN = 1000          # you requested 1000 â€” we keep it, but EARLY_STOP will cut if it stalls\nBATCH = 64\nCLASSIFIER_EPOCHS = 50\nSEED = 42\nPATIENCE_GAN = 120         # stop adversarial loop if gen loss doesn't improve for this many epochs\nSAVE_SIZE_CAP_BYTES = 2 * 1024**3   # 2 GiB safety cap (we'll warn if exceeded)\nLOG_EVERY = 50\n\nnp.random.seed(SEED); tf.random.set_seed(SEED)\n\n# -------- load data splits (Cell 1 saved .npz) --------\nX_train, X_test, y_train, y_test = load_data_splits()\nX = X_train.astype(np.float32)\nFEATURES = X.shape[1]\nN_train = X.shape[0]\nprint(f\"[GAN] features={FEATURES} | n_train={N_train}\")\n\n# -------- build generator & discriminator (unchanged arch) --------\ndef build_generator():\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(256, input_dim=LATENT_DIM),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dense(512),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dense(1024),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dense(FEATURES, activation='tanh')\n    ], name=\"generator\")\n\ndef build_discriminator():\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(512, input_shape=(FEATURES,)),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ], name=\"discriminator\")\n\ngenerator = build_generator()\ndiscriminator = build_discriminator()\n\nopt_d = tf.keras.optimizers.Adam(0.0002, 0.5)\nopt_g = tf.keras.optimizers.Adam(0.0002, 0.5)\n\n# -------- prepare GAN model (compile with disc non-trainable for GAN-object) --------\ndiscriminator.trainable = False\nz = tf.keras.Input(shape=(LATENT_DIM,))\ngan_out = discriminator(generator(z))\ngan = tf.keras.Model(z, gan_out, name=\"gan_model\")\ngan.compile(optimizer=opt_g, loss=\"binary_crossentropy\")\n\n# Now compile discriminator separately for manual training loops\ndiscriminator.trainable = True\ndiscriminator.compile(optimizer=opt_d, loss=\"binary_crossentropy\")\n\n# -------- tf.data pipeline (fast + memory-safe) --------\nAUTOTUNE = tf.data.AUTOTUNE\nshuffle_buffer = min(10000, N_train)\nds = tf.data.Dataset.from_tensor_slices(X).shuffle(shuffle_buffer, seed=SEED).repeat().batch(BATCH).prefetch(AUTOTUNE)\nds_iter = iter(ds)\n\nreal_label = tf.ones((BATCH,1), tf.float32)\nfake_label = tf.zeros((BATCH,1), tf.float32)\n\n# -------- training step (tf.function) --------\n@tf.function\ndef train_step(real_batch):\n    # discriminator on real\n    with tf.GradientTape() as tape_d_real:\n        pred_real = discriminator(real_batch, training=True)\n        loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_label, pred_real))\n    grads_real = tape_d_real.gradient(loss_real, discriminator.trainable_variables)\n    opt_d.apply_gradients(zip(grads_real, discriminator.trainable_variables))\n\n    # discriminator on fake\n    noise = tf.random.normal((BATCH, LATENT_DIM))\n    fake_batch = generator(noise, training=True)\n    with tf.GradientTape() as tape_d_fake:\n        pred_fake = discriminator(fake_batch, training=True)\n        loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(fake_label, pred_fake))\n    grads_fake = tape_d_fake.gradient(loss_fake, discriminator.trainable_variables)\n    opt_d.apply_gradients(zip(grads_fake, discriminator.trainable_variables))\n\n    # generator step (try to fool discriminator) â€” compute grads manually\n    noise2 = tf.random.normal((BATCH, LATENT_DIM))\n    with tf.GradientTape() as tape_g:\n        gen_out = generator(noise2, training=True)\n        disc_out_for_g = discriminator(gen_out, training=False)   # freeze disc for generator update\n        loss_g = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_label, disc_out_for_g))\n    grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n    opt_g.apply_gradients(zip(grads_g, generator.trainable_variables))\n\n    return (loss_real + loss_fake) * 0.5, loss_g\n\n# -------- adversarial training with early stop on generator loss --------\nprint(\"[GAN] Adversarial training started...\")\nd_losses, g_losses = [], []\nbest_g_loss = np.inf\nno_improve = 0\ngen_best_path = os.path.join(OUTPUT_DIR, \"generator_best.weights.h5\")\n\nstart_time = time.time()\nfor epoch in range(EPOCHS_GAN):\n    real_batch = next(ds_iter)\n    d_val, g_val = train_step(real_batch)\n\n    d_losses.append(float(d_val))\n    g_losses.append(float(g_val))\n\n    # check improvement & save best generator weights only\n    cur_g = float(g_val)\n    if cur_g < best_g_loss - 1e-8:\n        best_g_loss = cur_g\n        no_improve = 0\n        generator.save_weights(gen_best_path)   # tiny file\n    else:\n        no_improve += 1\n\n    # log sparse\n    if (epoch % LOG_EVERY) == 0 or epoch == EPOCHS_GAN-1:\n        elapsed = time.time() - start_time\n        print(f\"[Epoch {epoch}/{EPOCHS_GAN}] D_loss={d_losses[-1]:.4f} | G_loss={g_losses[-1]:.4f} | best_g={best_g_loss:.4f} | no_imp={no_improve} | elapsed={elapsed:.1f}s\")\n\n    # early stopping for GAN loop\n    if no_improve >= PATIENCE_GAN:\n        print(f\"[GAN] Early stopping: generator loss didn't improve for {PATIENCE_GAN} epochs (best_g={best_g_loss:.5f})\")\n        break\n\nprint(f\"[GAN] Adversarial loop done in {time.time()-start_time:.1f}s. Best G loss: {best_g_loss:.5f}\")\nprint(\"Generator best weights:\", gen_best_path)\n\n# -------- classifier fine-tune: discriminator as classifier (use streaming synthetic again) --------\ndef mixed_batch_generator(X_real, y_real, batch):\n    n = X_real.shape[0]\n    half = batch // 2\n    while True:\n        idx = np.random.randint(0, n, half)\n        real_x = X_real[idx]\n        real_y = y_real[idx].reshape(-1,1).astype(np.float32)\n\n        noise = np.random.normal(0,1,(half,LATENT_DIM)).astype(np.float32)\n        gen_x = generator.predict(noise, verbose=0)\n        gen_y = np.zeros((half,1), dtype=np.float32)\n\n        Xb = np.vstack([real_x, gen_x])\n        yb = np.vstack([real_y, gen_y])\n        perm = np.random.permutation(len(Xb))\n        yield Xb[perm], yb[perm]\n\ntrain_gen = mixed_batch_generator(X, y_train, BATCH)\nsteps_per_epoch = max(10, N_train // BATCH)\n\ndisc_best_path = os.path.join(OUTPUT_DIR, \"discriminator_best.weights.h5\")\n# ModelCheckpoint requires '.weights.h5' when save_weights_only=True\ncheckpoint = ModelCheckpoint(disc_best_path, monitor=\"val_loss\",\n                             save_best_only=True, save_weights_only=True, verbose=0)\nearly = EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n\ndiscriminator.trainable = True\ndiscriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0002,0.5),\n                      loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nprint(\"[Classifier] Fine-tuning discriminator using streamed synthetic batches...\")\nhistory = discriminator.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    epochs=CLASSIFIER_EPOCHS,\n    validation_data=(X_test.astype(np.float32), y_test.reshape(-1,1).astype(np.float32)),\n    callbacks=[checkpoint, early],\n    verbose=1\n)\n\nprint(\"Discriminator best weights:\", disc_best_path)\n\n# -------- evaluation & small PNGs (ROC + train curves) --------\ny_prob = discriminator.predict(X_test).ravel()\ny_pred = (y_prob >= 0.5).astype(int)\n\nprint(\"\\n=== Classification Report (Discriminator) ===\")\nprint(classification_report(y_test, y_pred))\n\n# ROC\ntry:\n    auc_val = roc_auc_score(y_test, y_prob)\nexcept Exception:\n    auc_val = None\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nplt.figure(figsize=(5,5)); plt.plot(fpr, tpr, label=f\"AUC={auc_val:.3f}\" if auc_val else \"ROC\"); plt.plot([0,1],[0,1],'k--', alpha=0.3)\nplt.title(\"Discriminator ROC\"); plt.legend(); plt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"gan_discriminator_roc.png\"), dpi=100); plt.close()\n\n# Train/val curves\nif history is not None:\n    plt.figure(figsize=(6,3))\n    plt.plot(history.history.get(\"accuracy\", []), label=\"train_acc\")\n    plt.plot(history.history.get(\"val_accuracy\", []), label=\"val_acc\")\n    plt.legend(); plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, \"gan_disc_acc.png\"), dpi=100); plt.close()\n\n    plt.figure(figsize=(6,3))\n    plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n    plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n    plt.legend(); plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, \"gan_disc_loss.png\"), dpi=100); plt.close()\n\n# -------- save JSON result entry (models_results.json via helper) --------\nres = make_result_dict(\"Vanilla_GAN_best_only\", discriminator, X_test, y_test, history)\nres.update({\n    \"best_generator_weights\": os.path.basename(gen_best_path),\n    \"best_discriminator_weights\": os.path.basename(disc_best_path),\n    \"gan_epochs_ran\": int(len(g_losses)),\n    \"timestamp\": time.time()\n})\nsave_model_result(res)\nprint(\"[JSON] result appended.\")\n\n# -------- storage guard: ensure saved files stay small (warn if > 2 GiB) --------\ndef human_mb(n): return f\"{n/1024**2:.2f} MB\"\ntotal_bytes = 0\nfor root,_,files in os.walk(OUTPUT_DIR):\n    for f in files:\n        total_bytes += os.path.getsize(os.path.join(root,f))\nif total_bytes > SAVE_SIZE_CAP_BYTES:\n    print(\"âš ï¸ WARNING: OUTPUT_DIR size > 2 GiB â€” consider removing large artifacts.\")\nprint(f\"Output directory size: {total_bytes/1024**2:.2f} MB\")\n\n# -------- cleanup --------\nK.clear_session()\ngc.collect()\n\nprint(\"âœ… CELL 3 finished â€” only BEST weights saved (generator + discriminator) + PNGs + JSON.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:33:41.836742Z","iopub.execute_input":"2025-11-23T20:33:41.837170Z","execution_failed":"2025-11-24T07:38:16.798Z"}},"outputs":[{"name":"stdout","text":"[GAN] features=128 | n_train=3089910\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1763930037.804977      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1763930037.807547      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"[GAN] Adversarial training started...\n[Epoch 0/1000] D_loss=0.8708 | G_loss=0.7148 | best_g=0.7148 | no_imp=0 | elapsed=4.4s\n[Epoch 50/1000] D_loss=0.5992 | G_loss=1.0553 | best_g=0.4685 | no_imp=45 | elapsed=4.9s\n[Epoch 100/1000] D_loss=0.6025 | G_loss=0.8002 | best_g=0.4685 | no_imp=95 | elapsed=5.2s\n[GAN] Early stopping: generator loss didn't improve for 120 epochs (best_g=0.46851)\n[GAN] Adversarial loop done in 5.4s. Best G loss: 0.46851\nGenerator best weights: /kaggle/working/dl_results/generator_best.weights.h5\n[Classifier] Fine-tuning discriminator using streamed synthetic batches...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1763930052.552337     127 service.cc:148] XLA service 0x799630009fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1763930052.553887     127 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1763930052.553910     127 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1763930052.761734     127 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1763930053.206399     127 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3272s\u001b[0m 68ms/step - accuracy: 0.7801 - loss: 0.3408 - val_accuracy: 0.6512 - val_loss: 0.5936\nEpoch 2/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3264s\u001b[0m 68ms/step - accuracy: 0.8188 - loss: 0.3033 - val_accuracy: 0.6786 - val_loss: 0.5615\nEpoch 3/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2993s\u001b[0m 62ms/step - accuracy: 0.8300 - loss: 0.2912 - val_accuracy: 0.7043 - val_loss: 0.5339\nEpoch 4/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2932s\u001b[0m 61ms/step - accuracy: 0.8359 - loss: 0.2841 - val_accuracy: 0.7099 - val_loss: 0.5244\nEpoch 5/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2927s\u001b[0m 61ms/step - accuracy: 0.8412 - loss: 0.2785 - val_accuracy: 0.7246 - val_loss: 0.5083\nEpoch 6/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2931s\u001b[0m 61ms/step - accuracy: 0.8444 - loss: 0.2743 - val_accuracy: 0.7330 - val_loss: 0.4997\nEpoch 7/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2930s\u001b[0m 61ms/step - accuracy: 0.8469 - loss: 0.2715 - val_accuracy: 0.7392 - val_loss: 0.4925\nEpoch 8/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2902s\u001b[0m 60ms/step - accuracy: 0.8491 - loss: 0.2691 - val_accuracy: 0.7420 - val_loss: 0.4873\nEpoch 9/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2978s\u001b[0m 62ms/step - accuracy: 0.8508 - loss: 0.2666 - val_accuracy: 0.7472 - val_loss: 0.4804\nEpoch 10/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2959s\u001b[0m 61ms/step - accuracy: 0.8521 - loss: 0.2645 - val_accuracy: 0.7529 - val_loss: 0.4736\nEpoch 11/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2883s\u001b[0m 60ms/step - accuracy: 0.8542 - loss: 0.2630 - val_accuracy: 0.7468 - val_loss: 0.4787\nEpoch 12/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3001s\u001b[0m 62ms/step - accuracy: 0.8545 - loss: 0.2616 - val_accuracy: 0.7577 - val_loss: 0.4678\nEpoch 13/50\n\u001b[1m31115/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[1m17:07\u001b[0m 60ms/step - accuracy: 0.8558 - loss: 0.2608","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ================================\n# CELL 3 â€” Vanilla GAN (Kaggle-optimized, faster)\n# - Keeps architecture identical\n# - Mixed precision, steps-based loop, faster IO\n# - Saves ONLY best generator + best discriminator weights (.weights.h5)\n# ================================\nimport os, time, gc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import backend as K\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score\n\n# ---------- CONFIG ----------\nOUTPUT_DIR = \"/kaggle/working/dl_results\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# keep your architecture / hyperparams unchanged (per request)\nLATENT_DIM = 100\nEPOCHS_GAN = 1000          # kept (we control runtime via patience & steps)\nBATCH = 64\nCLASSIFIER_EPOCHS = 50\nSEED = 42\n\n# speed-tuners\nPATIENCE_GAN = 30          # stop adversarial loop if gen loss doesn't improve for this many *steps*\nLOG_EVERY_STEPS = 50\nSAVE_SIZE_CAP_BYTES = 2 * 1024**3   # safety check\nUSE_MIXED_PRECISION = True  # ENABLE mixed-precision for T4\n\nnp.random.seed(SEED); tf.random.set_seed(SEED)\n\n# ---------- mixed precision (fast on T4) ----------\nif USE_MIXED_PRECISION:\n    try:\n        from tensorflow.keras import mixed_precision\n        mixed_precision.set_global_policy('mixed_float16')\n        print(\"[info] Mixed precision enabled (float16 compute).\")\n    except Exception as e:\n        print(\"[warn] cannot enable mixed precision:\", e)\n\n# ---------- load splits ----------\nX_train, X_test, y_train, y_test = load_data_splits()\nX = X_train.astype(np.float32)\nFEATURES = X.shape[1]\nN_train = X.shape[0]\nprint(f\"[GAN] features={FEATURES} | n_train={N_train}\")\n\n# ---------- model builders (unchanged) ----------\ndef build_generator():\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(256, input_dim=LATENT_DIM),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dense(512),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dense(1024),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dense(FEATURES, activation='tanh')\n    ], name=\"generator\")\n\ndef build_discriminator():\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(512, input_shape=(FEATURES,)),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256),\n        tf.keras.layers.LeakyReLU(0.2),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')  # ensure final logits in float32 for numeric stability\n    ], name=\"discriminator\")\n\ngenerator = build_generator()\ndiscriminator = build_discriminator()\n\nopt_d = tf.keras.optimizers.Adam(0.0002, 0.5)\nopt_g = tf.keras.optimizers.Adam(0.0002, 0.5)\n\n# ---------- compile GAN object (discriminator frozen inside gan) ----------\ndiscriminator.trainable = False\nz = tf.keras.Input(shape=(LATENT_DIM,))\ngan_out = discriminator(generator(z))\ngan = tf.keras.Model(z, gan_out, name=\"gan\")\ngan.compile(optimizer=opt_g, loss=\"binary_crossentropy\")\n\n# compile discriminator separately (trainable) for manual gradient steps\ndiscriminator.trainable = True\ndiscriminator.compile(optimizer=opt_d, loss=\"binary_crossentropy\")\n\n# ---------- tf.data pipeline ----------\nAUTOTUNE = tf.data.AUTOTUNE\nshuffle_buffer = min(10000, N_train)\nds = tf.data.Dataset.from_tensor_slices(X).shuffle(shuffle_buffer, seed=SEED).repeat().batch(BATCH).prefetch(AUTOTUNE)\nds_iter = iter(ds)\n\nreal_label = tf.ones((BATCH,1), tf.float32)\nfake_label = tf.zeros((BATCH,1), tf.float32)\n\n# ---------- tf.function train step (kept simple & fast) ----------\n@tf.function\ndef train_step(real_batch):\n    # discriminator on real\n    with tf.GradientTape() as tape_d_real:\n        pred_real = discriminator(real_batch, training=True)\n        loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_label, pred_real))\n    grads_real = tape_d_real.gradient(loss_real, discriminator.trainable_variables)\n    opt_d.apply_gradients(zip(grads_real, discriminator.trainable_variables))\n\n    # discriminator on fake\n    noise = tf.random.normal((BATCH, LATENT_DIM))\n    fake_batch = generator(noise, training=True)\n    with tf.GradientTape() as tape_d_fake:\n        pred_fake = discriminator(fake_batch, training=True)\n        loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(fake_label, pred_fake))\n    grads_fake = tape_d_fake.gradient(loss_fake, discriminator.trainable_variables)\n    opt_d.apply_gradients(zip(grads_fake, discriminator.trainable_variables))\n\n    # generator step\n    noise2 = tf.random.normal((BATCH, LATENT_DIM))\n    with tf.GradientTape() as tape_g:\n        gen_out = generator(noise2, training=True)\n        # ensure discriminator used in eval mode for generator update (no dropout effect)\n        disc_out_for_g = discriminator(gen_out, training=False)\n        loss_g = tf.reduce_mean(tf.keras.losses.binary_crossentropy(real_label, disc_out_for_g))\n    grads_g = tape_g.gradient(loss_g, generator.trainable_variables)\n    opt_g.apply_gradients(zip(grads_g, generator.trainable_variables))\n\n    # convert losses to float32 for Python side\n    return tf.cast((loss_real + loss_fake) * 0.5, tf.float32), tf.cast(loss_g, tf.float32)\n\n# ---------- steps control (faster monitoring) ----------\nprint(\"[GAN] Starting adversarial training (steps-based loop)...\")\nd_losses, g_losses = [], []\nbest_g_loss = np.inf\nno_improve = 0\ngen_best_path = os.path.join(OUTPUT_DIR, \"generator_best_main-3.weights.h5\")\n\nstart_time = time.time()\nmax_steps = EPOCHS_GAN * max(1, N_train // BATCH)  # upper bound steps; early stop will exit earlier\n# We WILL break early once PATIENCE_GAN is reached (no improvement on gen loss)\n\nfor step in range(int(max_steps)):\n    real_batch = next(ds_iter)\n    d_val, g_val = train_step(real_batch)\n\n    d_losses.append(float(d_val.numpy()))\n    g_losses.append(float(g_val.numpy()))\n\n    cur_g = g_losses[-1]\n    # only write weights when we have improvement to minimize IO\n    if cur_g < best_g_loss - 1e-8:\n        best_g_loss = cur_g\n        no_improve = 0\n        generator.save_weights(gen_best_path)\n    else:\n        no_improve += 1\n\n    # log occasionally\n    if (step % LOG_EVERY_STEPS) == 0:\n        elapsed = time.time() - start_time\n        steps_done = step + 1\n        # estimate remaining (very rough)\n        avg_step_time = elapsed / steps_done\n        est_remaining_s = avg_step_time * (max_steps - steps_done)\n        print(f\"[step {step}/{int(max_steps)}] D={d_losses[-1]:.4f} G={g_losses[-1]:.4f} bestG={best_g_loss:.4f} noImp={no_improve} elapsed={elapsed:.1f}s est_remain={est_remaining_s/60:.1f}m\")\n\n    # early-stop based on PATIENCE_GAN (counts steps)\n    if no_improve >= PATIENCE_GAN:\n        print(f\"[GAN] Early stop after {step+1} steps (no gen improvement for {PATIENCE_GAN} steps).\")\n        break\n\n# record number of GAN steps / equivalent epochs run\ngan_steps_ran = len(g_losses)\ngan_epochs_equiv = gan_steps_ran / max(1, N_train // BATCH)\nprint(f\"[GAN] Done. Steps ran: {gan_steps_ran} (~{gan_epochs_equiv:.2f} epochs). Best G loss: {best_g_loss:.5f}\")\nprint(\"Generator best weights saved to:\", gen_best_path)\n\n# ---------- classifier fine-tune (streamed synthetic batches) ----------\ndef mixed_batch_generator(X_real, y_real, batch):\n    n = X_real.shape[0]\n    half = batch // 2\n    while True:\n        idx = np.random.randint(0, n, half)\n        real_x = X_real[idx]\n        real_y = y_real[idx].reshape(-1,1).astype(np.float32)\n\n        noise = np.random.normal(0,1,(half, LATENT_DIM)).astype(np.float32)\n        gen_x = generator.predict(noise, verbose=0)\n        gen_y = np.zeros((half,1), dtype=np.float32)\n\n        Xb = np.vstack([real_x, gen_x])\n        yb = np.vstack([real_y, gen_y])\n        perm = np.random.permutation(len(Xb))\n        yield Xb[perm], yb[perm]\n\ntrain_gen = mixed_batch_generator(X, y_train, BATCH)\nsteps_per_epoch = max(10, N_train // BATCH)\n\ndisc_best_path = os.path.join(OUTPUT_DIR, \"discriminator_best_main-3.weights.h5\")\ncheckpoint = ModelCheckpoint(disc_best_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True, verbose=0)\nearly = EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n\ndiscriminator.trainable = True\ndiscriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0002,0.5),\n                      loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nprint(\"[Classifier] Fine-tuning discriminator using streamed synthetic batches...\")\nhistory = discriminator.fit(\n    train_gen,\n    steps_per_epoch = steps_per_epoch,\n    epochs = CLASSIFIER_EPOCHS,\n    validation_data = (X_test.astype(np.float32), y_test.reshape(-1,1).astype(np.float32)),\n    callbacks = [checkpoint, early],\n    verbose = 1\n)\nprint(\"Discriminator best weights saved to:\", disc_best_path)\n\n# ---------- evaluation & plots ----------\ny_prob = discriminator.predict(X_test).ravel()\ny_pred = (y_prob >= 0.5).astype(int)\nprint(\"\\n=== Classification Report ===\")\nprint(classification_report(y_test, y_pred))\n\n# ROC + small plots\ntry:\n    auc_val = roc_auc_score(y_test, y_prob)\nexcept Exception:\n    auc_val = None\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nplt.figure(figsize=(5,5)); plt.plot(fpr, tpr, label=f\"AUC={auc_val:.3f}\" if auc_val else \"ROC\"); plt.plot([0,1],[0,1],'k--', alpha=0.3)\nplt.title(\"Discriminator ROC\"); plt.legend(); plt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"gan_discriminator_roc_main-2.png\"), dpi=100); plt.close()\n\nif history is not None:\n    plt.figure(figsize=(6,3)); plt.plot(history.history.get(\"accuracy\",[])); plt.plot(history.history.get(\"val_accuracy\",[]))\n    plt.title(\"Train/Val Acc\"); plt.savefig(os.path.join(OUTPUT_DIR, \"gan_disc_acc_main-2.png\"), dpi=100); plt.close()\n\n    plt.figure(figsize=(6,3)); plt.plot(history.history.get(\"loss\",[])); plt.plot(history.history.get(\"val_loss\",[]))\n    plt.title(\"Train/Val Loss\"); plt.savefig(os.path.join(OUTPUT_DIR, \"gan_disc_loss_main-2.png\"), dpi=100); plt.close()\n\n# ---------- save JSON result ----------\nres = make_result_dict(\"Vanilla_GAN_fast\", discriminator, X_test, y_test, history)\nres.update({\n    \"best_generator_weights\": os.path.basename(gen_best_path),\n    \"best_discriminator_weights\": os.path.basename(disc_best_path),\n    \"gan_steps_ran\": int(gan_steps_ran),\n    \"gan_epochs_equiv\": float(gan_epochs_equiv),\n    \"timestamp\": time.time()\n})\nsave_model_result(res)\nprint(\"[JSON] result appended.\")\n\n# ---------- storage guard ----------\ndef bytes_to_mb(n): return n/1024**2\ntotal_bytes = 0\nfor root,_,files in os.walk(OUTPUT_DIR):\n    for f in files:\n        total_bytes += os.path.getsize(os.path.join(root,f))\nprint(f\"Output dir size: {bytes_to_mb(total_bytes):.2f} MB\")\nif total_bytes > SAVE_SIZE_CAP_BYTES:\n    print(\"âš ï¸ OUTPUT_DIR > 2 GiB - remove artifacts or reduce checkpoints.\")\n\n# ---------- cleanup ----------\nK.clear_session()\ngc.collect()\nprint(\"âœ… CELL 3 complete. Only best weights + PNGs + JSON saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:27:23.755003Z","iopub.execute_input":"2025-11-24T14:27:23.755776Z","execution_failed":"2025-11-25T02:14:09.981Z"}},"outputs":[{"name":"stdout","text":"[info] Mixed precision enabled (float16 compute).\n[GAN] features=128 | n_train=3089910\n[GAN] Starting adversarial training (steps-based loop)...\n[step 0/48279000] D=0.8566 G=0.7086 bestG=0.7086 noImp=0 elapsed=2.5s est_remain=1980658.3m\n[GAN] Early stop after 35 steps (no gen improvement for 30 steps).\n[GAN] Done. Steps ran: 35 (~0.00 epochs). Best G loss: 0.47509\nGenerator best weights saved to: /kaggle/working/dl_results/generator_best_main-3.weights.h5\n[Classifier] Fine-tuning discriminator using streamed synthetic batches...\nEpoch 1/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2807s\u001b[0m 58ms/step - accuracy: 0.7814 - loss: 0.3374 - val_accuracy: 0.6534 - val_loss: 0.5921\nEpoch 2/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2795s\u001b[0m 58ms/step - accuracy: 0.8190 - loss: 0.3031 - val_accuracy: 0.6785 - val_loss: 0.5602\nEpoch 3/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2797s\u001b[0m 58ms/step - accuracy: 0.8301 - loss: 0.2908 - val_accuracy: 0.7017 - val_loss: 0.5353\nEpoch 4/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2797s\u001b[0m 58ms/step - accuracy: 0.8361 - loss: 0.2841 - val_accuracy: 0.7134 - val_loss: 0.5202\nEpoch 5/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2822s\u001b[0m 58ms/step - accuracy: 0.8410 - loss: 0.2785 - val_accuracy: 0.7211 - val_loss: 0.5107\nEpoch 6/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2803s\u001b[0m 58ms/step - accuracy: 0.8449 - loss: 0.2743 - val_accuracy: 0.7325 - val_loss: 0.4983\nEpoch 7/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2816s\u001b[0m 58ms/step - accuracy: 0.8470 - loss: 0.2714 - val_accuracy: 0.7376 - val_loss: 0.4919\nEpoch 8/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2798s\u001b[0m 58ms/step - accuracy: 0.8490 - loss: 0.2690 - val_accuracy: 0.7443 - val_loss: 0.4846\nEpoch 9/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2804s\u001b[0m 58ms/step - accuracy: 0.8511 - loss: 0.2665 - val_accuracy: 0.7466 - val_loss: 0.4812\nEpoch 10/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2791s\u001b[0m 58ms/step - accuracy: 0.8521 - loss: 0.2648 - val_accuracy: 0.7513 - val_loss: 0.4747\nEpoch 11/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2772s\u001b[0m 57ms/step - accuracy: 0.8535 - loss: 0.2628 - val_accuracy: 0.7514 - val_loss: 0.4707\nEpoch 12/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2771s\u001b[0m 57ms/step - accuracy: 0.8549 - loss: 0.2609 - val_accuracy: 0.7589 - val_loss: 0.4680\nEpoch 13/50\n\u001b[1m48279/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2773s\u001b[0m 57ms/step - accuracy: 0.8562 - loss: 0.2598 - val_accuracy: 0.7563 - val_loss: 0.4640\nEpoch 14/50\n\u001b[1m33136/48279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”\u001b[0m \u001b[1m14:06\u001b[0m 56ms/step - accuracy: 0.8572 - loss: 0.2587","output_type":"stream"}],"execution_count":null}]}